{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Getting Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 111]\n",
      "[nltk_data]     Connection refused>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer,BertForPreTraining, BertConfig\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "import re\n",
    "from scipy.spatial.distance import  cosine\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "DATA_DIR = Path('data')\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "SEEDS = ['finance', 'medicine', 'sports', 'technology']\n",
    "NUM_WORDS_PER_SET = 10\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def get_embeddings(model, tokens, embedding_size=768):\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "        embedding = output.last_hidden_state[0][1]\n",
    "        return torch.reshape(embedding, (embedding_size, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_embeddings_batch(model, tokens, embedding_size=768, batch_size=4):\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "        embedding = output.last_hidden_state[:, 1, :]\n",
    "        return embedding\n",
    "\n",
    "\n",
    "def cos_distance_batch(topic, words):\n",
    "    return np.inner(\n",
    "        topic, words) / (np.linalg.norm(topic) * np.linalg.norm(words, axis=1))\n",
    "\n",
    "\n",
    "def job(vocab, topic, tokenizer, model, batch_size=4):\n",
    "    res_col = np.zeros((len(vocab), ))\n",
    "    vocab = list(vocab)\n",
    "    loop = tqdm(range(0, len(vocab), batch_size))\n",
    "    loop.set_description(f\"topic: {topic}\")\n",
    "    topic_token = tokenizer(topic,\n",
    "                            return_tensors='pt',\n",
    "                            padding=True,\n",
    "                            max_length=10,\n",
    "                            truncation=True)\n",
    "    topic_emb = get_embeddings(model, topic_token)\n",
    "    for batch_index in loop:\n",
    "        lo = batch_index\n",
    "        hi = min(batch_index + batch_size, len(vocab))\n",
    "        batch = vocab[batch_index:batch_index + batch_size]\n",
    "        tokens = tokenizer(batch,\n",
    "                           return_tensors='pt',\n",
    "                           padding='max_length',\n",
    "                           max_length=10,\n",
    "                           truncation=True)\n",
    "        # if len(token['input_ids']) > 3:\n",
    "        #     print(f\"WARNING: Word '{word}' is not in BERT's vocabulary\")\n",
    "        word_embs = get_embeddings_batch(model, tokens)\n",
    "        res_col[lo:hi] = cos_distance_batch(topic_emb, word_embs)\n",
    "        # res_col.append(cosine(topic_emb, word_emb))\n",
    "    return res_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "token = tokenizer(\"medicine\", return_tensors='pt', truncation=True)\n",
    "print(token.input_ids.shape)\n",
    "out = model(**token)\n",
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer('[MASK]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cat_emb = get_embeddings(cat_outputs)\n",
    "hello_emb = get_embeddings(hello_outputs)\n",
    "hi_emb = get_embeddings(hi_outputs)\n",
    "\n",
    "print(cosine(hi_emb, hello_emb))\n",
    "print(cosine(hi_emb, cat_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cosine([1], [0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Building vocab for the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv(Path('data') / 'content_cleaned.txt')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def vocab_preprocess(row, lemmatize=True):\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    row = row.lower().strip()\n",
    "    words = row.split(' ')\n",
    "    # Remove stop words\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    res = {}\n",
    "    for word in words:\n",
    "        count = res.get(word, 0)\n",
    "        count += 1\n",
    "        res[word] = count\n",
    "    if (lemmatize):\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MIN_COUNT=3\n",
    "f = open('data/content_cleaned.txt')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "data = []\n",
    "vocab = {}\n",
    "for i, row in tqdm(enumerate(lines)):\n",
    "    freq = vocab_preprocess(row, False)\n",
    "    for _word, _count in freq.items():\n",
    "        count = vocab.get(_word, 0) + _count\n",
    "        vocab[_word] = count\n",
    "uncommon_words = []\n",
    "for word, count in tqdm(vocab.items()):\n",
    "    if count < MIN_COUNT:\n",
    "       uncommon_words.append(word)\n",
    "print(len(uncommon_words))\n",
    "print(len(vocab.keys()))\n",
    "for word in uncommon_words:\n",
    "    del vocab[word]\n",
    "print(len(vocab.keys()))\n",
    "with open(DATA_DIR / \"vocab\" / \"global_vocab_no_lemmatize.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocab, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build global rankings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vocab = None\n",
    "with open(DATA_DIR / \"vocab\" / \"global_vocab_no_lemmatize.pkl\", \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "print(len(vocab))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "topic_embeddings = []\n",
    "cols = []\n",
    "for seed in SEEDS:\n",
    "    cols.append(job(vocab, seed, tokenizer, model, batch_size=128))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "arr = np.array(cols)\n",
    "res_dict = {\n",
    "    '_vocab': list(vocab),\n",
    "}\n",
    "for i, topic in enumerate(SEEDS):\n",
    "    print(arr[i].shape)\n",
    "    res_dict[topic] = arr[i]\n",
    "res_df = pd.DataFrame(res_dict)\n",
    "res_df = res_df.set_index(['_vocab'])\n",
    "res_df.to_csv('./results/global_cos_similarity.csv')\n",
    "res_df.sort_values('finance', ascending=False).head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('./results/global_cos_similarity.csv', index_col='_vocab')\n",
    "for seed in SEEDS:\n",
    "    line = df.sort_values(seed, ascending=False).head(4).index[1::]\n",
    "    print(\" \".join(line))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res_df.sort_values(by='technology', ascending=False).head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Initialize word sets from $e$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "res_df = pd.read_csv('global_cos_similarity.csv')\n",
    "res_df = res_df.set_index(['_vocab'])\n",
    "word_set = {}\n",
    "added_words = set()\n",
    "for seed in SEEDS:\n",
    "    word_set[seed] = []\n",
    "    col = res_df[seed]\n",
    "    sorted_col = col.sort_values()[::-1]\n",
    "    i = 0\n",
    "    for word, _ in sorted_col.iteritems():\n",
    "        if i == NUM_WORDS_PER_SET + 1:\n",
    "            break\n",
    "        if word not in added_words:\n",
    "            word_set[seed].append(word)\n",
    "            added_words.add(word)\n",
    "            i += 1\n",
    "\n",
    "word_set_df = pd.DataFrame(word_set)\n",
    "word_set_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for topic, words in word_set.items():\n",
    "    print(f\"{' '.join(words[1:4])}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Local Knowlege using pretrained BERT"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "from transformers import BertTokenizer, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15,\n",
    ")\n",
    "dataset = load_dataset('ag_news')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "concatenate_datasets([dataset['train'], dataset['test']], axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python3 scripts/train.py \\\n",
    "    -o ./models \\\n",
    "    -t ./data/tokens/tokens-pretrained-30522.pkl \\\n",
    "    -n bert-pretrined-30522 "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compute Local Cosine similarity"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('models/bert-pretrained-pretrained-30522-2022-06-22-10-34/checkpoint-10000')\n",
    "model.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "with open('data/vocab/global_vocab_no_lemmatize.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "len(vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_embeddings_batch(model, tokens, embedding_size=768, batch_size=4):\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "        embedding = output.last_hidden_state[:, 1, :]\n",
    "        return embedding\n",
    "\n",
    "\n",
    "def cos_distance_batch(topic, words):\n",
    "    return np.inner(topic, words) / (np.linalg.norm(topic) * np.linalg.norm(words, axis=1))\n",
    "\n",
    "\n",
    "def job(vocab, topic, tokenizer, model, batch_size=4):\n",
    "    res_col = np.zeros((len(vocab), ))\n",
    "    vocab = list(vocab)\n",
    "    loop = tqdm(range(0, len(vocab), batch_size))\n",
    "    loop.set_description(f\"topic: {topic}\")\n",
    "    topic_token = tokenizer(topic, return_tensors='pt', padding=True, max_length=10, truncation=True)\n",
    "    topic_emb = get_embeddings(model, topic_token)\n",
    "    for batch_index in loop:\n",
    "        lo = batch_index\n",
    "        hi = min(batch_index + batch_size, len(vocab))\n",
    "        batch = vocab[batch_index:batch_index + batch_size]\n",
    "        tokens = tokenizer(batch, return_tensors='pt', padding='max_length', max_length=10, truncation=True)\n",
    "        # if len(token['input_ids']) > 3:\n",
    "        #     print(f\"WARNING: Word '{word}' is not in BERT's vocabulary\")\n",
    "        word_embs = get_embeddings_batch(model, tokens)\n",
    "        res_col[lo:hi] = cos_distance_batch(topic_emb, word_embs)\n",
    "        # res_col.append(cosine(topic_emb, word_emb))\n",
    "    return res_col"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res = []\n",
    "for topic in SEEDS:\n",
    "    res_col = job(vocab, topic, tokenizer, model, batch_size=256)\n",
    "    res.append(res_col)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "arr = np.array(res)\n",
    "arr = arr.T\n",
    "res_dict = {    \n",
    "    '_vocab': list(vocab),\n",
    "}\n",
    "for i, topic in enumerate(SEEDS):\n",
    "    res_dict[topic] = arr[:, i]\n",
    "res_df = pd.DataFrame(res_dict)\n",
    "res_df = res_df.set_index(['_vocab'])\n",
    "res_df.to_csv('local_embeddings_bert-pretrained-pretrained.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res_df = pd.read_csv('./local_embeddings_bert-pretrained-pretrained.csv', index_col='_vocab')\n",
    "res_df.sort_values('finance', ascending=False).head(10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# BERTopic"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "seeds = np.array([SEEDS]).T.tolist()\n",
    "topic_model = BERTopic(seed_topic_list=seeds)\n",
    "docs = pd.read_csv(DATA_DIR / 'dataset.csv')['description']\n",
    "topic_model.fit_transform(docs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "similar_topics, similarity = topic_model.find_topics(\"finance\", top_n=5)\n",
    "for i, topic in enumerate(similar_topics):\n",
    "    print(topic_model.get_topic(topic)[0][0], similarity[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CatE\n",
    "**kwown issue**: Seg Fault when seed words > 3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "local_emb_vocab_df = pd.read_csv('./results/CatE/emb_seeds_w.txt', sep=' ', skiprows=[0], index_col=0, header=None)\n",
    "local_emb_vocab_df.dropna(axis=1, inplace=True)\n",
    "\n",
    "local_emb_topic_df = pd.read_csv('./results/CatE/emb_seeds_t.txt', sep=' ', skiprows=[0], index_col=0, header=None)\n",
    "local_emb_topic_df.dropna(axis=1, inplace=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_cate_cos_similarity(vocab, vocab_embs, topic_emb, topic, batch_size=32, show_progress=True):\n",
    "    res_col = np.zeros((len(vocab), ))\n",
    "    loop = range(0, len(vocab), batch_size)\n",
    "    if show_progress:\n",
    "        loop = tqdm(loop)\n",
    "        loop.set_description(f\"topic: {topic}\")\n",
    "    for batch_index in loop:\n",
    "        lo = batch_index\n",
    "        hi = min(batch_index + batch_size, len(vocab))\n",
    "\n",
    "        batch = vocab[lo:hi]\n",
    "        batch_embs = vocab_embs.loc[batch, :]\n",
    "        res_col[lo:hi] = cos_distance_batch(topic_emb, batch_embs)\n",
    "        # res_col.append(cosine(topic_emb, word_emb))\n",
    "    return res_col"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vocab_f = open('./data/vocab/global_vocab_no_lemmatize.pkl', 'rb')\n",
    "vocab = pickle.load(vocab_f)\n",
    "vocab_f.close()\n",
    "not_in_vocab_words = []\n",
    "for word in vocab.keys():\n",
    "    if word not in local_emb_vocab_df.index:\n",
    "        not_in_vocab_words.append(word)\n",
    "print(len(not_in_vocab_words))\n",
    "for word in not_in_vocab_words:\n",
    "    del vocab[word]\n",
    "for word in local_emb_vocab_df.index:\n",
    "    if word not in vocab:\n",
    "        local_emb_vocab_df =  local_emb_vocab_df.drop(word, axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res = {}\n",
    "res['_vocab'] = vocab.keys()\n",
    "for topic in local_emb_topic_df.index:\n",
    "    topic_emb = local_emb_topic_df.loc[topic, :]\n",
    "    res_col = compute_cate_cos_similarity(list(vocab.keys()), local_emb_vocab_df, topic_emb, topic, show_progress=False)\n",
    "    res[topic] = res_col\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(res)\n",
    "res_df = res_df.set_index(['_vocab'])\n",
    "res_df.to_csv('./results/cate_local_embeddings.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compute Ensemble ranking [Proposed]\n",
    "Which does not work as expected -> CatE has different embeddings for the same word in topic and vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "local_emb_vocab_df = pd.read_csv('./results/CatE/emb_seeds_w.txt', sep=' ', skiprows=[0], index_col=0, header=None)\n",
    "local_emb_vocab_df.dropna(axis=1, inplace=True)\n",
    "\n",
    "local_emb_topic_df = pd.read_csv('./results/CatE/emb_seeds_t.txt', sep=' ', skiprows=[0], index_col=0, header=None)\n",
    "local_emb_topic_df.dropna(axis=1, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_vocab_cos_similarity(emb):\n",
    "    out_shape = (emb.shape[0], emb.shape[0])\n",
    "    df = pd.DataFrame(np.zeros((out_shape)), index=emb.index, columns=emb.index)\n",
    "    for word in tqdm(emb.index):\n",
    "        topic_emb = emb.loc[word, :]\n",
    "        res_row = cos_distance_batch(topic_emb, emb)\n",
    "        df.loc[word, :] = res_row\n",
    "    return df\n",
    "res = compute_vocab_cos_similarity(local_emb_vocab_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "res.loc['accounting', 'unumprovident']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cosine(local_emb_vocab_df.loc['accounting', :], local_emb_topic_df.loc['accounting', :])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compute emsemble ranking [CatE]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "global_cos_df = pd.read_csv('./results/global_cos_similarity.csv', index_col=0)\n",
    "local_cos_df = pd.read_csv('./results/cate_local_embeddings.csv', index_col=0)\n",
    "# min_max_scalar\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def ensemble_ranking(score_g, score_l, rho):\n",
    "    exponent = 1 / rho\n",
    "    base = 0.5 * np.power(1/score_g, rho) + 0.5 * np.power(1/score_l, rho) \n",
    "    return 1/np.power(base, exponent)\n",
    "\n",
    "def scale_data(data):\n",
    "    scaler = MinMaxScaler()\n",
    "    return scaler.fit_transform(data)\n",
    "\n",
    "def compute_vocab_ensemble_rankings(score_l_df, score_g_df, vocab, rho=1): \n",
    "    raw = np.zeros((len(vocab), score_l_df.shape[1])).astype(np.double)\n",
    "    res = pd.DataFrame(raw, index=vocab, columns=score_g_df.columns)\n",
    "    for topic_idx in range(score_l_df.shape[1]):\n",
    "        score_l = score_l_df.loc[vocab, score_l_df.columns[topic_idx]]\n",
    "        score_g = score_g_df.loc[vocab, score_g_df.columns[topic_idx]]\n",
    "        res.iloc[:, topic_idx] = ensemble_ranking(score_l, score_g, rho)\n",
    "    return res\n",
    "s_global_cos_df = scale_data(global_cos_df)\n",
    "local_cos_df.loc[:, :] = scale_data(local_cos_df)\n",
    "global_cos_df.loc[:, :] = scale_data(global_cos_df)\n",
    "res = compute_vocab_ensemble_rankings(local_cos_df, global_cos_df, local_cos_df.index)\n",
    "res['finance'].sort_values(ascending=False)\n",
    "res.to_csv('./results/ensemble/cate_ensemble_rankings.csv') "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Compute emsemble ranking [BERT-pretrained]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "global_cos_df = pd.read_csv('./results/global_cos_similarities.csv', index_col=0)\n",
    "local_cos_df = pd.read_csv('./results/bert/bert-pretrained.csv', index_col=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def ensemble_ranking(score_g, score_l, rho):\n",
    "    exponent = 1 / rho\n",
    "    base = 0.4 * np.power(1/score_g, rho) + 0.6 * np.power(1/score_l, rho) \n",
    "    return 1/np.power(base, exponent)\n",
    "\n",
    "def compute_vocab_ensemble_rankings(score_l_df, score_g_df, vocab, rho=0.5): \n",
    "    raw = np.zeros((len(vocab), score_l_df.shape[1])).astype(np.double)\n",
    "    res = pd.DataFrame(raw, index=vocab, columns=score_g_df.columns)\n",
    "    for topic_idx in range(score_l_df.shape[1]):\n",
    "        score_l = score_l_df.loc[vocab, score_l_df.columns[topic_idx]]\n",
    "        score_g = score_g_df.loc[vocab, score_g_df.columns[topic_idx]]\n",
    "        res.iloc[:, topic_idx] = ensemble_ranking(score_l, score_g, rho)\n",
    "    return res\n",
    "res = compute_vocab_ensemble_rankings(global_cos_df, local_cos_df, local_cos_df.index)\n",
    "res['finance'].sort_values(ascending=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build occurance matrix"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from tqdm.auto import tqdm\n",
    "with open('./data/vocab/global_vocab_no_lemmatize.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "data = np.zeros((len(vocab), len(vocab)))\n",
    "cooccur = pd.DataFrame(columns=vocab.keys(), index=vocab.keys(), data=data)\n",
    "with open('./data/content_cleaned.txt') as f:\n",
    "    lines =  f.readlines()\n",
    "    for line in tqdm(lines):\n",
    "        words = [word for word in line.split(' ') if word in vocab.keys()]\n",
    "        for word_x in words: \n",
    "            for word_y in words:\n",
    "                if word_x != word_y:\n",
    "                    cooccur.loc[word_x, word_y] += 1\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cooccur['finance', 'banking']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cooccur.loc['finance','finance']\n",
    "cooccur.to_csv('./data/vocab/cooccurence_matrix_quick.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cooccur_new.loc['finance', 'finance']\n",
    "cooccur_new.to_csv('./data/vocab/cooccurence_matrix.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Build PMI"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "cooccur_mat = pd.read_csv('./data/vocab/cooccurence_matrix-agnews.csv', index_col=0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores_df = pd.read_csv(\n",
    "    \"results/ensemble/ensemble_score_bert-pretrained_2022-07-03-13-12.csv\", index_col='_vocab')\n",
    "SEEDS = ['finance', 'sports', 'medicine', 'technology']\n",
    "res = {}\n",
    "for topic in SEEDS:\n",
    "    res[topic] = list(scores_df[topic].sort_values(ascending=False).head(5).index)\n",
    "res"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def prob_word(word, mat):\n",
    "    return np.sum(mat[word]) / np.sum(mat.to_numpy())\n",
    "def prob_cooccur(a, b, mat):\n",
    "    return mat.loc[a, b] / np.sum(mat.to_numpy())\n",
    "def pmi(a, b, mat):\n",
    "    prob_a=prob_word(a, mat)\n",
    "    prob_b=prob_word(b, mat)\n",
    "    prob_ab_cooccur = prob_cooccur(a,b, mat)\n",
    "    return prob_ab_cooccur / (prob_a * prob_b)\n",
    "pmis = {}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pmis = {}\n",
    "for topic in SEEDS:\n",
    "    pmis[topic] = np.sum([pmi(word, topic, cooccur_mat) for word in res[topic]])\n",
    "pmis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cooccur_mat.loc['medicine', 'pathology']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "local_emb_vocab_df = pd.read_csv('./results/CatE/emb_seeds_w.txt', sep=' ', skiprows=[0], index_col=0, header=None)\n",
    "local_emb_vocab_df.dropna(axis=1, inplace=True)\n",
    "\n",
    "local_emb_topic_df = pd.read_csv('./results/CatE/emb_seeds_t.txt', sep=' ', skiprows=[0], index_col=0, header=None)\n",
    "local_emb_topic_df.dropna(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faace728ad9d41199d3d786c18a24e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_vocab_cos_similarity(emb):\n",
    "    out_shape = (emb.shape[0], emb.shape[0])\n",
    "    df = pd.DataFrame(np.zeros((out_shape)), index=emb.index, columns=emb.index)\n",
    "    for word in tqdm(emb.index):\n",
    "        topic_emb = emb.loc[word, :]\n",
    "        res_row = cos_distance_batch(topic_emb, emb)\n",
    "        df.loc[word, :] = res_row\n",
    "    return df\n",
    "res = compute_vocab_cos_similarity(local_emb_vocab_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5662511389486806"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.loc['accounting', 'unumprovident']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33416083167506283"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(local_emb_vocab_df.loc['accounting', :], local_emb_topic_df.loc['accounting', :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Compute emsemble ranking [CatE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "global_cos_df = pd.read_csv('./results/global_cos_similarity.csv', index_col=0)\n",
    "local_cos_df = pd.read_csv('./results/cate_local_embeddings.csv', index_col=0)\n",
    "# min_max_scalar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ensemble_ranking(score_g, score_l, rho):\n",
    "    exponent = 1 / rho\n",
    "    base = 0.5 * np.power(1/score_g, rho) + 0.5 * np.power(1/score_l, rho) \n",
    "    return 1/np.power(base, exponent)\n",
    "\n",
    "def scale_data(data):\n",
    "    scaler = MinMaxScaler()\n",
    "    return scaler.fit_transform(data)\n",
    "\n",
    "def compute_vocab_ensemble_rankings(score_l_df, score_g_df, vocab, rho=1): \n",
    "    raw = np.zeros((len(vocab), score_l_df.shape[1])).astype(np.double)\n",
    "    res = pd.DataFrame(raw, index=vocab, columns=score_g_df.columns)\n",
    "    for topic_idx in range(score_l_df.shape[1]):\n",
    "        score_l = score_l_df.loc[vocab, score_l_df.columns[topic_idx]]\n",
    "        score_g = score_g_df.loc[vocab, score_g_df.columns[topic_idx]]\n",
    "        res.iloc[:, topic_idx] = ensemble_ranking(score_l, score_g, rho)\n",
    "    return res\n",
    "s_global_cos_df = scale_data(global_cos_df)\n",
    "local_cos_df.loc[:, :] = scale_data(local_cos_df)\n",
    "global_cos_df.loc[:, :] = scale_data(global_cos_df)\n",
    "res = compute_vocab_ensemble_rankings(local_cos_df, global_cos_df, local_cos_df.index)\n",
    "res['finance'].sort_values(ascending=False)\n",
    "res.to_csv('./results/ensemble/cate_ensemble_rankings.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Compute emsemble ranking [BERT-pretrained]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "global_cos_df = pd.read_csv('./results/global_cos_similarities.csv', index_col=0)\n",
    "local_cos_df = pd.read_csv('./results/bert/bert-pretrained.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_vocab\n",
       "finance       1.000000\n",
       "banking       0.768596\n",
       "securities    0.746399\n",
       "financing     0.737215\n",
       "finances      0.736829\n",
       "                ...   \n",
       "olof          0.089758\n",
       "indah         0.088084\n",
       "obasanjo      0.086977\n",
       "morten        0.083190\n",
       "winky         0.068860\n",
       "Name: finance, Length: 25877, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ensemble_ranking(score_g, score_l, rho):\n",
    "    exponent = 1 / rho\n",
    "    base = 0.4 * np.power(1/score_g, rho) + 0.6 * np.power(1/score_l, rho) \n",
    "    return 1/np.power(base, exponent)\n",
    "\n",
    "def compute_vocab_ensemble_rankings(score_l_df, score_g_df, vocab, rho=0.5): \n",
    "    raw = np.zeros((len(vocab), score_l_df.shape[1])).astype(np.double)\n",
    "    res = pd.DataFrame(raw, index=vocab, columns=score_g_df.columns)\n",
    "    for topic_idx in range(score_l_df.shape[1]):\n",
    "        score_l = score_l_df.loc[vocab, score_l_df.columns[topic_idx]]\n",
    "        score_g = score_g_df.loc[vocab, score_g_df.columns[topic_idx]]\n",
    "        res.iloc[:, topic_idx] = ensemble_ranking(score_l, score_g, rho)\n",
    "    return res\n",
    "res = compute_vocab_ensemble_rankings(global_cos_df, local_cos_df, local_cos_df.index)\n",
    "res['finance'].sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Build occurance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1441c0e41334468795028d53c547df23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/127600 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from tqdm.auto import tqdm\n",
    "with open('./data/vocab/global_vocab_no_lemmatize.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "data = np.zeros((len(vocab), len(vocab)))\n",
    "cooccur = pd.DataFrame(columns=vocab.keys(), index=vocab.keys(), data=data)\n",
    "with open('./data/content_cleaned.txt') as f:\n",
    "    lines =  f.readlines()\n",
    "    for line in tqdm(lines):\n",
    "        words = [word for word in line.split(' ') if word in vocab.keys()]\n",
    "        for word_x in words: \n",
    "            for word_y in words:\n",
    "                if word_x != word_y:\n",
    "                    cooccur.loc[word_x, word_y] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "('finance', 'banking')",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/honours/lib/python3.8/site-packages/pandas/core/indexes/base.py:3621\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3620\u001B[0m \u001B[39mtry\u001B[39;00m:\n\u001B[0;32m-> 3621\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49m_engine\u001B[39m.\u001B[39;49mget_loc(casted_key)\n\u001B[1;32m   3622\u001B[0m \u001B[39mexcept\u001B[39;00m \u001B[39mKeyError\u001B[39;00m \u001B[39mas\u001B[39;00m err:\n",
      "File \u001B[0;32m~/miniconda3/envs/honours/lib/python3.8/site-packages/pandas/_libs/index.pyx:136\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/miniconda3/envs/honours/lib/python3.8/site-packages/pandas/_libs/index.pyx:163\u001B[0m, in \u001B[0;36mpandas._libs.index.IndexEngine.get_loc\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001B[0m, in \u001B[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mKeyError\u001B[0m: ('finance', 'banking')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "\u001B[1;32m/home/alexzhang/code/github.com/zhangchi0104/honours-sgtm/experiment.ipynb Cell 29'\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bhome-desktop/home/alexzhang/code/github.com/zhangchi0104/honours-sgtm/experiment.ipynb#ch0000028vscode-remote?line=0'>1</a>\u001B[0m cooccur[\u001B[39m'\u001B[39;49m\u001B[39mfinance\u001B[39;49m\u001B[39m'\u001B[39;49m, \u001B[39m'\u001B[39;49m\u001B[39mbanking\u001B[39;49m\u001B[39m'\u001B[39;49m]\n",
      "File \u001B[0;32m~/miniconda3/envs/honours/lib/python3.8/site-packages/pandas/core/frame.py:3505\u001B[0m, in \u001B[0;36mDataFrame.__getitem__\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m   3503\u001B[0m \u001B[39mif\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39mcolumns\u001B[39m.\u001B[39mnlevels \u001B[39m>\u001B[39m \u001B[39m1\u001B[39m:\n\u001B[1;32m   3504\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_getitem_multilevel(key)\n\u001B[0;32m-> 3505\u001B[0m indexer \u001B[39m=\u001B[39m \u001B[39mself\u001B[39;49m\u001B[39m.\u001B[39;49mcolumns\u001B[39m.\u001B[39;49mget_loc(key)\n\u001B[1;32m   3506\u001B[0m \u001B[39mif\u001B[39;00m is_integer(indexer):\n\u001B[1;32m   3507\u001B[0m     indexer \u001B[39m=\u001B[39m [indexer]\n",
      "File \u001B[0;32m~/miniconda3/envs/honours/lib/python3.8/site-packages/pandas/core/indexes/base.py:3623\u001B[0m, in \u001B[0;36mIndex.get_loc\u001B[0;34m(self, key, method, tolerance)\u001B[0m\n\u001B[1;32m   3621\u001B[0m     \u001B[39mreturn\u001B[39;00m \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_engine\u001B[39m.\u001B[39mget_loc(casted_key)\n\u001B[1;32m   3622\u001B[0m \u001B[39mexcept\u001B[39;00m \u001B[39mKeyError\u001B[39;00m \u001B[39mas\u001B[39;00m err:\n\u001B[0;32m-> 3623\u001B[0m     \u001B[39mraise\u001B[39;00m \u001B[39mKeyError\u001B[39;00m(key) \u001B[39mfrom\u001B[39;00m \u001B[39merr\u001B[39;00m\n\u001B[1;32m   3624\u001B[0m \u001B[39mexcept\u001B[39;00m \u001B[39mTypeError\u001B[39;00m:\n\u001B[1;32m   3625\u001B[0m     \u001B[39m# If we have a listlike key, _check_indexing_error will raise\u001B[39;00m\n\u001B[1;32m   3626\u001B[0m     \u001B[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001B[39;00m\n\u001B[1;32m   3627\u001B[0m     \u001B[39m#  the TypeError.\u001B[39;00m\n\u001B[1;32m   3628\u001B[0m     \u001B[39mself\u001B[39m\u001B[39m.\u001B[39m_check_indexing_error(key)\n",
      "\u001B[0;31mKeyError\u001B[0m: ('finance', 'banking')"
     ]
    }
   ],
   "source": [
    "cooccur['finance', 'banking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cooccur.loc['finance','finance']\n",
    "cooccur.to_csv('./data/vocab/cooccurence_matrix_quick.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cooccur_new.loc['finance', 'finance']\n",
    "cooccur_new.to_csv('./data/vocab/cooccurence_matrix.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Build PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "cooccur_mat = pd.read_csv('./data/vocab/cooccurence_matrix-agnews.csv', index_col=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'finance': ['finance', 'banking', 'securities', 'logistics', 'finances'],\n",
       " 'sports': ['sports', 'sport', 'basketball', 'arts', 'athletics'],\n",
       " 'medicine': ['medicine', 'medical', 'healthcare', 'dentistry', 'health'],\n",
       " 'technology': ['technology',\n",
       "  'technologies',\n",
       "  'industry',\n",
       "  'engineering',\n",
       "  'robotics']}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_df = pd.read_csv(\n",
    "    \"results/ensemble/ensemble_score_bert-pretrained_2022-07-03-13-12.csv\", index_col='_vocab')\n",
    "SEEDS = ['finance', 'sports', 'medicine', 'technology']\n",
    "res = {}\n",
    "for topic in SEEDS:\n",
    "    res[topic] = list(scores_df[topic].sort_values(ascending=False).head(5).index)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def prob_word(word, mat):\n",
    "    return np.sum(mat[word]) / np.sum(mat.to_numpy())\n",
    "def prob_cooccur(a, b, mat):\n",
    "    return mat.loc[a, b] / np.sum(mat.to_numpy())\n",
    "def pmi(a, b, mat):\n",
    "    prob_a=prob_word(a, mat)\n",
    "    prob_b=prob_word(b, mat)\n",
    "    prob_ab_cooccur = prob_cooccur(a,b, mat)\n",
    "    return prob_ab_cooccur / (prob_a * prob_b)\n",
    "pmis = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'finance': 12.716741869602656,\n",
       " 'sports': 23.57696338135249,\n",
       " 'medicine': 4.184382179804778,\n",
       " 'technology': 31.939528634506942}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pmis = {}\n",
    "for topic in SEEDS:\n",
    "    pmis[topic] = np.sum([pmi(word, topic, cooccur_mat) for word in res[topic]])\n",
    "pmis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cooccur_mat.loc['medicine', 'pathology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('honours')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "eea59d2da9fcf6d5a89ffa791376363fdbff07d1b7c37705fe62005135f4eda1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}