{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Getting Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer,BertForPreTraining, BertConfig\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "import re\n",
    "from scipy.spatial.distance import  cosine\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "DATA_DIR = Path('data')\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "SEEDS = ['finance', 'medicine', 'sports', 'technology']\n",
    "NUM_WORDS_PER_SET = 10\n",
    "\n",
    "\n",
    "def get_embeddings(model, tokens, embedding_size=768):\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "        embedding = output.last_hidden_state[0][1]\n",
    "        return torch.reshape(embedding, (embedding_size, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "tokens = {\n",
    "    \"medicine\": tokenizer(\"medicine\", return_tensors='pt', truncation=True),\n",
    "    \"cat\": tokenizer(\"cat\", return_tensors='pt', truncation=True),\n",
    "    \"kitty\": tokenizer('kitty', return_tensors='pt', truncation=True),\n",
    "    \"feline\": tokenizer('feline', return_tensors='pt', truncation=True),\n",
    "    \"doctor\": tokenizer(\"doctor\", return_tensors='pt', truncation=True),\n",
    "    \"medical\": tokenizer(\"medical\", return_tensors='pt', truncation=True),\n",
    "}\n",
    "embeddings = {}\n",
    "for key, val in tokens.items():\n",
    "    embeddings[key] = get_embeddings(model, val)\n",
    "\n",
    "print(\"cat-kitty: \" + f\"{cosine(embeddings['cat'], embeddings['kitty'])}\")\n",
    "print('cat-doctor: ' + f\"{cosine(embeddings['cat'], embeddings['doctor'])}\")\n",
    "print('medicine-cat: ' +\n",
    "      f\"{cosine(embeddings['medicine'], embeddings['cat'])}\")\n",
    "print('medicine-medical: ' +\n",
    "      f\"{cosine(embeddings['medicine'], embeddings['medical'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer('[MASK]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cat_emb = get_embeddings(cat_outputs)\n",
    "hello_emb = get_embeddings(hello_outputs)\n",
    "hi_emb = get_embeddings(hi_outputs)\n",
    "\n",
    "print(cosine(hi_emb, hello_emb))\n",
    "print(cosine(hi_emb, cat_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cosine([1], [0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Building vocab for the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(Path('data') / 'dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt.zip')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords.zip')\n",
    "except:\n",
    "    nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def vocab_preprocess(row, lemmatize=True):\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    row = row.lower()\n",
    "    row = row.split('-')[1::]\n",
    "    row = ''.join(row)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # tokenize words\n",
    "    words = re.findall(re.compile('[a-zA-Z]+'), row)\n",
    "    # Remove stop words\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    if (lemmatize):\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "desc = df['description'].astype(str)\n",
    "data = []\n",
    "for i, row in desc.iteritems():\n",
    "    data.append(vocab_preprocess(row))\n",
    "vocab = set()\n",
    "for words in data:\n",
    "    vocab = vocab.union(set(words))\n",
    "with open(\"vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Build global rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vocab = None\n",
    "with open('vocab.pkl', \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "topic_embeddings = []\n",
    "for seed in SEEDS:\n",
    "    topic_embeddings.append(get_embeddings(model, tokenizer(seed, return_tensors='pt')))\n",
    "\n",
    "# df_dict = { \"_vocab\": list(vocab) }\n",
    "# for seed in SEEDS:\n",
    "#     df_dict[seed] = [np.NaN for _ in range(len(vocab))]\n",
    "# cos_scores = pd.DataFrame(df_dict)\n",
    "# cos_scores = cos_scores.set_index(['_vocab'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for word in tqdm(vocab):\n",
    "    token = tokenizer(word, return_tensors='pt')\n",
    "    if len(token['input_ids']) > 3:\n",
    "        print(f\"WARNING: Word '{word}' is not in BERT's vocabulary\")\n",
    "    word_emb = get_embeddings(model, token)\n",
    "    res_row = []\n",
    "    for topic in topic_embeddings:\n",
    "        res_row.append(cosine(topic, word_emb))\n",
    "    res.append(res_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "arr = 1 - np.array(res)\n",
    "res_dict = {\n",
    "    '_vocab': list(vocab),\n",
    "}\n",
    "for i, topic in enumerate(SEEDS):\n",
    "    res_dict[topic] = arr[:, i]\n",
    "res_df = pd.DataFrame(res_dict)\n",
    "res_df = res_df.set_index(['_vocab'])\n",
    "res_df.to_csv('global_cos_similarity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Initialize word sets from $e$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "res_df = pd.read_csv('global_cos_similarity.csv')\n",
    "res_df = res_df.set_index(['_vocab'])\n",
    "word_set = {}\n",
    "added_words = set()\n",
    "for seed in SEEDS:\n",
    "    word_set[seed] = []\n",
    "    col = res_df[seed]\n",
    "    sorted_col = col.sort_values()[::-1]\n",
    "    i = 0\n",
    "    for word, _ in sorted_col.iteritems():\n",
    "        if i == NUM_WORDS_PER_SET + 1:\n",
    "            break\n",
    "        if word not in added_words:\n",
    "            word_set[seed].append(word)\n",
    "            added_words.add(word)\n",
    "            i += 1\n",
    "\n",
    "pd.DataFrame(word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# build local embeddings\n",
    "## 1. Train local embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Train tokenizers\n",
    "\n",
    "dataset = pd.read_csv(DATA_DIR / 'dataset.csv')\n",
    "dataset['description'].values.tofile('./data/content.txt', sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "sentence_a = []\n",
    "sentence_b = []\n",
    "label = []\n",
    "news = dataset['description']\n",
    "for i, paragraph in dataset['description'].iteritems():\n",
    "    sentences = [\n",
    "        sentence for sentence in paragraph.split('.') if sentence != ''\n",
    "    ]\n",
    "    num_sentences = len(sentences)\n",
    "    if num_sentences > 1:\n",
    "        start = random.randint(0, num_sentences-2)\n",
    "        # 50/50 whether is IsNextSentence or NotNextSentence\n",
    "        if random.random() >= 0.5:\n",
    "            # this is IsNextSentence\n",
    "            sentence_a.append(sentences[start])\n",
    "            sentence_b.append(sentences[start+1])\n",
    "            label.append(0)\n",
    "        else:\n",
    "            index = random.randint(0, dataset.shape[0] - 1)\n",
    "            # this is NotNextSentence\n",
    "            sentence_a.append(sentences[start])\n",
    "            sentence_b.append(dataset['description'][index])\n",
    "            label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tokenizer(sentence_a, sentence_b, return_tensors='pt',\n",
    "                   max_length=512, truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inputs['next_sentence_label'] = torch.LongTensor([label]).T\n",
    "inputs['labels'] = inputs.input_ids.detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create random array of floats with equal dimensions to input_ids tensor\n",
    "rand = torch.rand(inputs.input_ids.shape)\n",
    "# create mask array\n",
    "mask_arr = (rand < 0.15) * (inputs.input_ids != 101) * \\\n",
    "           (inputs.input_ids != 102) * (inputs.input_ids != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "selection = []\n",
    "\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    selection.append(\n",
    "        torch.flatten(mask_arr[i].nonzero()).tolist()\n",
    "    )\n",
    "for i in range(inputs.input_ids.shape[0]):\n",
    "    inputs.input_ids[i, selection[i]] = 103\n",
    "with open('raw_inputs.pkl', 'wb') as f:\n",
    "    pickle.dump(inputs, f)\n",
    "inputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.eoncodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.eoncodings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return {key: torch.tensor(val[i]) for key, val in self.eoncodings.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset = MyDataset(inputs)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "model = BertForPreTraining.from_pretrained('bert-base-uncased')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# initialize optimizer\n",
    "optim = AdamW(model.parameters(), lr=5e-5)\n",
    "from tqdm.notebook import tqdm  # for our progress bar\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        next_sentence_label = batch['next_sentence_label'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        # process\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        token_type_ids=token_type_ids,\n",
    "                        next_sentence_label=next_sentence_label,\n",
    "                        labels=labels)\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('tokens.pkl', 'rb') as f:\n",
    "    tokens = pickle.load(f)\n",
    "tokens['input_ids'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load local knowledge BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('models/bert-scratch-scratch-25000')\n",
    "model.eval()\n",
    "tokenizer = BertTokenizer(vocab_file='./data/vocab/bert-local-25000-vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def job(vocab, topic, tokenizer, model):\n",
    "    res_col = []\n",
    "    loop = tqdm(vocab)\n",
    "    loop.set_description(f\"topic: {topic}\")\n",
    "    topic_token = tokenizer(topic, return_tensors='pt')\n",
    "    topic_emb = get_embeddings(model, topic_token)\n",
    "    for word in loop:\n",
    "        token = tokenizer(word, return_tensors='pt')\n",
    "        if len(token['input_ids']) > 3:\n",
    "            print(f\"WARNING: Word '{word}' is not in BERT's vocabulary\")\n",
    "        word_emb = get_embeddings(model, token)\n",
    "        res_col.append(cosine(topic_emb, word_emb))\n",
    "    return res_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for topic in SEEDS:\n",
    "    res_col = job(vocab, topic, tokenizer, model)\n",
    "    res.append(res_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "arr = np.array(res)\n",
    "arr = arr.T\n",
    "res_dict = {\n",
    "    '_vocab': list(vocab),\n",
    "}\n",
    "for i, topic in enumerate(SEEDS):\n",
    "    res_dict[topic] = arr[:, i]\n",
    "res_df = pd.DataFrame(res_dict)\n",
    "res_df = res_df.set_index(['_vocab'])\n",
    "res_df.sort_values('technology').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "'asteroidlike' in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "token_struct = tokenizer(\"inoculate\")\n",
    "token_struct\n",
    "with open('./data/vocab/bert-local-30522-vocab.txt') as f:\n",
    "    raw_vocab = f.readlines()\n",
    "for token in token_struct['input_ids']:\n",
    "    print(raw_vocab[token])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "seeds = np.array([SEEDS]).T.tolist()\n",
    "topic_model = BERTopic(seed_topic_list=seeds)\n",
    "docs = pd.read_csv(DATA_DIR / 'dataset.csv')['description']\n",
    "topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()\n",
    "similar_topics, similarity = topic_model.find_topics(\"finance\", top_n=5)\n",
    "topic_model.get_topic(similar_topics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = np.array([SEEDS]).T\n",
    "seeds.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "41b1468c2a8aaa770bdb45a83028fbdb5ae0291ada001c7cfb7400daf117e2e6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('honours')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
