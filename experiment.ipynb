{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Getting Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer,BertForPreTraining, BertConfig\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "import re\n",
    "from scipy.spatial.distance import  cosine\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "DATA_DIR = Path('data')\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "SEEDS = ['finance', 'medicine', 'sports', 'technology']\n",
    "NUM_WORDS_PER_SET = 10\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def get_embeddings(model, tokens, embedding_size=768):\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "        embedding = output.last_hidden_state[0][1]\n",
    "        return torch.reshape(embedding, (embedding_size, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tokens = {\n",
    "    \"medicine\": tokenizer(\"medicine\", return_tensors='pt', truncation=True),\n",
    "    \"cat\": tokenizer(\"cat\", return_tensors='pt', truncation=True),\n",
    "    \"kitty\": tokenizer('kitty', return_tensors='pt', truncation=True),\n",
    "    \"feline\": tokenizer('feline', return_tensors='pt', truncation=True),\n",
    "    \"doctor\": tokenizer(\"doctor\", return_tensors='pt', truncation=True),\n",
    "    \"medical\": tokenizer(\"medical\", return_tensors='pt', truncation=True),\n",
    "}\n",
    "embeddings = {}\n",
    "for key, val in tokens.items():\n",
    "    embeddings[key] = get_embeddings(model, val)\n",
    "\n",
    "print(\"cat-kitty: \" + f\"{cosine(embeddings['cat'], embeddings['kitty'])}\")\n",
    "print('cat-doctor: ' + f\"{cosine(embeddings['cat'], embeddings['doctor'])}\")\n",
    "print('medicine-cat: ' +\n",
    "      f\"{cosine(embeddings['medicine'], embeddings['cat'])}\")\n",
    "print('medicine-medical: ' +\n",
    "      f\"{cosine(embeddings['medicine'], embeddings['medical'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer('[MASK]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cat_emb = get_embeddings(cat_outputs)\n",
    "hello_emb = get_embeddings(hello_outputs)\n",
    "hi_emb = get_embeddings(hi_outputs)\n",
    "\n",
    "print(cosine(hi_emb, hello_emb))\n",
    "print(cosine(hi_emb, cat_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cosine([1], [0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Building vocab for the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                              title  \\\n",
       "0         3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1         3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2         3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3         3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4         3  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                         description  \n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
       "1  Reuters - Private investment firm Carlyle Grou...  \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...  \n",
       "3  Reuters - Authorities have halted oil export\\f...  \n",
       "4  AFP - Tearaway world oil prices, toppling reco...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(Path('data') / 'dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/zhang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/zhang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/zhang/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt.zip')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords.zip')\n",
    "except:\n",
    "    nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def vocab_preprocess(row, lemmatize=True):\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    row = row.lower()\n",
    "    row = row.split('-')[1::]\n",
    "    row = ''.join(row)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # tokenize words\n",
    "    words = re.findall(re.compile('[a-zA-Z]+'), row)\n",
    "    # Remove stop words\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    if (lemmatize):\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "desc = df['description'].astype(str)\n",
    "data = []\n",
    "for i, row in desc.iteritems():\n",
    "    data.append(vocab_preprocess(row))\n",
    "vocab = set()\n",
    "for words in data:\n",
    "    vocab = vocab.union(set(words))\n",
    "with open(DATA_DIR / \"vocab\" / \"global_vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Build global rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45394\n"
     ]
    }
   ],
   "source": [
    "vocab = None\n",
    "with open(DATA_DIR / \"vocab\" / \"global_vocab.pkl\", \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "topic_embeddings = []\n",
    "for seed in SEEDS:\n",
    "    topic_embeddings.append(get_embeddings(model, tokenizer(seed, return_tensors='pt')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20128bfd292c40098b76422501307a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = []\n",
    "for word in tqdm(vocab):\n",
    "    token = tokenizer(word, return_tensors='pt')\n",
    "    if len(token['input_ids']) > 3:\n",
    "        print(f\"WARNING: Word '{word}' is not in BERT's vocabulary\")\n",
    "    word_emb = get_embeddings(model, token)\n",
    "    res_row = []\n",
    "    for topic in topic_embeddings:\n",
    "        res_row.append(cosine(topic, word_emb))\n",
    "    res.append(res_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "arr = 1 - np.array(res)\n",
    "res_dict = {\n",
    "    '_vocab': list(vocab),\n",
    "}\n",
    "for i, topic in enumerate(SEEDS):\n",
    "    res_dict[topic] = arr[:, i]\n",
    "res_df = pd.DataFrame(res_dict)\n",
    "res_df = res_df.set_index(['_vocab'])\n",
    "res_df.to_csv(DATA_DIR / 'global_cos_similarity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>finance</th>\n",
       "      <th>medicine</th>\n",
       "      <th>sports</th>\n",
       "      <th>technology</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_vocab</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>finance</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.723560</td>\n",
       "      <td>0.561142</td>\n",
       "      <td>0.688010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accounting</th>\n",
       "      <td>0.852870</td>\n",
       "      <td>0.753559</td>\n",
       "      <td>0.544790</td>\n",
       "      <td>0.710905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistics</th>\n",
       "      <td>0.841821</td>\n",
       "      <td>0.743687</td>\n",
       "      <td>0.547152</td>\n",
       "      <td>0.750695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marketing</th>\n",
       "      <td>0.836664</td>\n",
       "      <td>0.715334</td>\n",
       "      <td>0.589341</td>\n",
       "      <td>0.735878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>banking</th>\n",
       "      <td>0.831688</td>\n",
       "      <td>0.732591</td>\n",
       "      <td>0.513997</td>\n",
       "      <td>0.692174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>procurement</th>\n",
       "      <td>0.786893</td>\n",
       "      <td>0.691719</td>\n",
       "      <td>0.476832</td>\n",
       "      <td>0.664342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>financial</th>\n",
       "      <td>0.777519</td>\n",
       "      <td>0.647263</td>\n",
       "      <td>0.509762</td>\n",
       "      <td>0.705595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>debt</th>\n",
       "      <td>0.773262</td>\n",
       "      <td>0.659070</td>\n",
       "      <td>0.459897</td>\n",
       "      <td>0.635410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>engineering</th>\n",
       "      <td>0.773229</td>\n",
       "      <td>0.726308</td>\n",
       "      <td>0.627797</td>\n",
       "      <td>0.805677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advertising</th>\n",
       "      <td>0.769661</td>\n",
       "      <td>0.681448</td>\n",
       "      <td>0.512919</td>\n",
       "      <td>0.688041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              finance  medicine    sports  technology\n",
       "_vocab                                               \n",
       "finance      1.000000  0.723560  0.561142    0.688010\n",
       "accounting   0.852870  0.753559  0.544790    0.710905\n",
       "logistics    0.841821  0.743687  0.547152    0.750695\n",
       "marketing    0.836664  0.715334  0.589341    0.735878\n",
       "banking      0.831688  0.732591  0.513997    0.692174\n",
       "procurement  0.786893  0.691719  0.476832    0.664342\n",
       "financial    0.777519  0.647263  0.509762    0.705595\n",
       "debt         0.773262  0.659070  0.459897    0.635410\n",
       "engineering  0.773229  0.726308  0.627797    0.805677\n",
       "advertising  0.769661  0.681448  0.512919    0.688041"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.sort_values(by='finance', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Initialize word sets from $e$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>finance</th>\n",
       "      <th>medicine</th>\n",
       "      <th>sports</th>\n",
       "      <th>technology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>finance</td>\n",
       "      <td>medicine</td>\n",
       "      <td>sport</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accounting</td>\n",
       "      <td>pharmacy</td>\n",
       "      <td>baseball</td>\n",
       "      <td>robotics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logistics</td>\n",
       "      <td>pathology</td>\n",
       "      <td>basketball</td>\n",
       "      <td>telecommunication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>marketing</td>\n",
       "      <td>medical</td>\n",
       "      <td>athletics</td>\n",
       "      <td>communication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>banking</td>\n",
       "      <td>biology</td>\n",
       "      <td>gymnastics</td>\n",
       "      <td>journalism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>procurement</td>\n",
       "      <td>surgery</td>\n",
       "      <td>volleyball</td>\n",
       "      <td>philosophy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>financial</td>\n",
       "      <td>health</td>\n",
       "      <td>football</td>\n",
       "      <td>industrial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>debt</td>\n",
       "      <td>astronomy</td>\n",
       "      <td>swimming</td>\n",
       "      <td>drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>engineering</td>\n",
       "      <td>industry</td>\n",
       "      <td>hockey</td>\n",
       "      <td>integration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>advertising</td>\n",
       "      <td>nutrition</td>\n",
       "      <td>education</td>\n",
       "      <td>sociology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>management</td>\n",
       "      <td>architecture</td>\n",
       "      <td>broadcasting</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        finance      medicine        sports         technology\n",
       "0       finance      medicine         sport         technology\n",
       "1    accounting      pharmacy      baseball           robotics\n",
       "2     logistics     pathology    basketball  telecommunication\n",
       "3     marketing       medical     athletics      communication\n",
       "4       banking       biology    gymnastics         journalism\n",
       "5   procurement       surgery    volleyball         philosophy\n",
       "6     financial        health      football         industrial\n",
       "7          debt     astronomy      swimming              drama\n",
       "8   engineering      industry        hockey        integration\n",
       "9   advertising     nutrition     education          sociology\n",
       "10   management  architecture  broadcasting            culture"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "res_df = pd.read_csv('global_cos_similarity.csv')\n",
    "res_df = res_df.set_index(['_vocab'])\n",
    "word_set = {}\n",
    "added_words = set()\n",
    "for seed in SEEDS:\n",
    "    word_set[seed] = []\n",
    "    col = res_df[seed]\n",
    "    sorted_col = col.sort_values()[::-1]\n",
    "    i = 0\n",
    "    for word, _ in sorted_col.iteritems():\n",
    "        if i == NUM_WORDS_PER_SET + 1:\n",
    "            break\n",
    "        if word not in added_words:\n",
    "            word_set[seed].append(word)\n",
    "            added_words.add(word)\n",
    "            i += 1\n",
    "\n",
    "pd.DataFrame(word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Knowlege using pretrained BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/home/zhang/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa8c6e5626f41fd8cdedb1a9b55afeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare datasets\n",
    "from transformers import BertTokenizer, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15,\n",
    ")\n",
    "dataset = load_dataset('ag_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 127600\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenate_datasets([dataset['train'], dataset['test']], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python3 scripts/train.py \\\n",
    "    -o ./models \\\n",
    "    -t ./data/tokens/tokens-pretrained-30522.pkl \\\n",
    "    -n bert-pretrined-30522 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Load local knowledge BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at models/bert-pretrained-pretrained were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('models/bert-pretrained-pretrained')\n",
    "model.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def job(vocab, topic, tokenizer, model):\n",
    "    res_col = []\n",
    "    loop = tqdm(vocab)\n",
    "    loop.set_description(f\"topic: {topic}\")\n",
    "    topic_token = tokenizer(topic, return_tensors='pt')\n",
    "    topic_emb = get_embeddings(model, topic_token)\n",
    "    for word in loop:\n",
    "        token = tokenizer(word, return_tensors='pt')\n",
    "        if len(token['input_ids']) > 3:\n",
    "            print(f\"WARNING: Word '{word}' is not in BERT's vocabulary\")\n",
    "        word_emb = get_embeddings(model, token)\n",
    "        res_col.append(cosine(topic_emb, word_emb))\n",
    "    return res_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73e04a14cca4916822fb94f33bb68c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b1eefa06204927bee1b8bf300ff4f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/zhang/code/github.com/honours-sgtm/experiment.ipynb Cell 29'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/zhang/code/github.com/honours-sgtm/experiment.ipynb#ch0000024vscode-remote?line=0'>1</a>\u001b[0m res \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/zhang/code/github.com/honours-sgtm/experiment.ipynb#ch0000024vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m topic \u001b[39min\u001b[39;00m SEEDS:\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/zhang/code/github.com/honours-sgtm/experiment.ipynb#ch0000024vscode-remote?line=2'>3</a>\u001b[0m     res_col \u001b[39m=\u001b[39m job(vocab, topic, tokenizer, model)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/zhang/code/github.com/honours-sgtm/experiment.ipynb#ch0000024vscode-remote?line=3'>4</a>\u001b[0m     res\u001b[39m.\u001b[39mappend(res_col)\n",
      "\u001b[1;32m/home/zhang/code/github.com/honours-sgtm/experiment.ipynb Cell 28'\u001b[0m in \u001b[0;36mjob\u001b[0;34m(vocab, topic, tokenizer, model)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/zhang/code/github.com/honours-sgtm/experiment.ipynb#ch0000023vscode-remote?line=8'>9</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(token[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/zhang/code/github.com/honours-sgtm/experiment.ipynb#ch0000023vscode-remote?line=9'>10</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWARNING: Word \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mword\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m is not in BERT\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms vocabulary\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/zhang/code/github.com/honours-sgtm/experiment.ipynb#ch0000023vscode-remote?line=10'>11</a>\u001b[0m     word_emb \u001b[39m=\u001b[39m get_embeddings(model, token)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/zhang/code/github.com/honours-sgtm/experiment.ipynb#ch0000023vscode-remote?line=11'>12</a>\u001b[0m     res_col\u001b[39m.\u001b[39mappend(cosine(topic_emb, word_emb))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/zhang/code/github.com/honours-sgtm/experiment.ipynb#ch0000023vscode-remote?line=12'>13</a>\u001b[0m \u001b[39mreturn\u001b[39;00m res_col\n",
      "\u001b[1;32m/home/zhang/code/github.com/honours-sgtm/experiment.ipynb Cell 2'\u001b[0m in \u001b[0;36mget_embeddings\u001b[0;34m(model, tokens, embedding_size)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/zhang/code/github.com/honours-sgtm/experiment.ipynb#ch0000001vscode-remote?line=20'>21</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_embeddings\u001b[39m(model, tokens, embedding_size\u001b[39m=\u001b[39m\u001b[39m768\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/zhang/code/github.com/honours-sgtm/experiment.ipynb#ch0000001vscode-remote?line=21'>22</a>\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/zhang/code/github.com/honours-sgtm/experiment.ipynb#ch0000001vscode-remote?line=22'>23</a>\u001b[0m         output \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtokens)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/zhang/code/github.com/honours-sgtm/experiment.ipynb#ch0000001vscode-remote?line=23'>24</a>\u001b[0m         embedding \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mlast_hidden_state[\u001b[39m0\u001b[39m][\u001b[39m1\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/zhang/code/github.com/honours-sgtm/experiment.ipynb#ch0000001vscode-remote?line=24'>25</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mreshape(embedding, (embedding_size, ))\n",
      "File \u001b[0;32m~/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:995\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=985'>986</a>\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=987'>988</a>\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=988'>989</a>\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=989'>990</a>\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=992'>993</a>\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=993'>994</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=994'>995</a>\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=995'>996</a>\u001b[0m     embedding_output,\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=996'>997</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=997'>998</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=998'>999</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=999'>1000</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1000'>1001</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1001'>1002</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1002'>1003</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1003'>1004</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1004'>1005</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1005'>1006</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1006'>1007</a>\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=1007'>1008</a>\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:582\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=572'>573</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=573'>574</a>\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=574'>575</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=578'>579</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=579'>580</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=580'>581</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=581'>582</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=582'>583</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=583'>584</a>\u001b[0m         attention_mask,\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=584'>585</a>\u001b[0m         layer_head_mask,\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=585'>586</a>\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=586'>587</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=587'>588</a>\u001b[0m         past_key_value,\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=588'>589</a>\u001b[0m         output_attentions,\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=589'>590</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=591'>592</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=592'>593</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:510\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=506'>507</a>\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=507'>508</a>\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=509'>510</a>\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=510'>511</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=511'>512</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=512'>513</a>\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=514'>515</a>\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/modeling_utils.py:2330\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/modeling_utils.py?line=2326'>2327</a>\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/modeling_utils.py?line=2327'>2328</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[0;32m-> <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/modeling_utils.py?line=2329'>2330</a>\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
      "File \u001b[0;32m~/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:522\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=520'>521</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[0;32m--> <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=521'>522</a>\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mintermediate(attention_output)\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=522'>523</a>\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=523'>524</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
      "File \u001b[0;32m~/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py:425\u001b[0m, in \u001b[0;36mBertIntermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=423'>424</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[0;32m--> <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=424'>425</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=425'>426</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py?line=426'>427</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/linear.py?line=101'>102</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> <a href='file:///home/zhang/miniconda3/envs/honours/lib/python3.8/site-packages/torch/nn/modules/linear.py?line=102'>103</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for topic in SEEDS:\n",
    "    res_col = job(vocab, topic, tokenizer, model)\n",
    "    res.append(res_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>finance</th>\n",
       "      <th>medicine</th>\n",
       "      <th>sports</th>\n",
       "      <th>technology</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_vocab</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>medicine</th>\n",
       "      <td>0.323753</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435356</td>\n",
       "      <td>0.226050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psychiatry</th>\n",
       "      <td>0.278695</td>\n",
       "      <td>0.135984</td>\n",
       "      <td>0.464712</td>\n",
       "      <td>0.258565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purification</th>\n",
       "      <td>0.360454</td>\n",
       "      <td>0.143276</td>\n",
       "      <td>0.460375</td>\n",
       "      <td>0.244680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wine</th>\n",
       "      <td>0.298581</td>\n",
       "      <td>0.146669</td>\n",
       "      <td>0.465945</td>\n",
       "      <td>0.242062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spear</th>\n",
       "      <td>0.353989</td>\n",
       "      <td>0.148336</td>\n",
       "      <td>0.470348</td>\n",
       "      <td>0.342442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cure</th>\n",
       "      <td>0.362462</td>\n",
       "      <td>0.150394</td>\n",
       "      <td>0.495592</td>\n",
       "      <td>0.292631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anthropology</th>\n",
       "      <td>0.316390</td>\n",
       "      <td>0.151040</td>\n",
       "      <td>0.422313</td>\n",
       "      <td>0.272204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>seal</th>\n",
       "      <td>0.377228</td>\n",
       "      <td>0.151204</td>\n",
       "      <td>0.478585</td>\n",
       "      <td>0.286898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genetics</th>\n",
       "      <td>0.292066</td>\n",
       "      <td>0.151213</td>\n",
       "      <td>0.408178</td>\n",
       "      <td>0.304390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rocket</th>\n",
       "      <td>0.329482</td>\n",
       "      <td>0.151498</td>\n",
       "      <td>0.431690</td>\n",
       "      <td>0.244864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               finance  medicine    sports  technology\n",
       "_vocab                                                \n",
       "medicine      0.323753  0.000000  0.435356    0.226050\n",
       "psychiatry    0.278695  0.135984  0.464712    0.258565\n",
       "purification  0.360454  0.143276  0.460375    0.244680\n",
       "wine          0.298581  0.146669  0.465945    0.242062\n",
       "spear         0.353989  0.148336  0.470348    0.342442\n",
       "cure          0.362462  0.150394  0.495592    0.292631\n",
       "anthropology  0.316390  0.151040  0.422313    0.272204\n",
       "seal          0.377228  0.151204  0.478585    0.286898\n",
       "genetics      0.292066  0.151213  0.408178    0.304390\n",
       "rocket        0.329482  0.151498  0.431690    0.244864"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "arr = np.array(res)\n",
    "arr = arr.T\n",
    "res_dict = {\n",
    "    '_vocab': list(vocab),\n",
    "}\n",
    "for i, topic in enumerate(SEEDS):\n",
    "    res_dict[topic] = arr[:, i]\n",
    "res_df = pd.DataFrame(res_dict)\n",
    "res_df = res_df.set_index(['_vocab'])\n",
    "res_df.to_csv('local_embeddings_bert.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "seeds = np.array([SEEDS]).T.tolist()\n",
    "topic_model = BERTopic(seed_topic_list=seeds)\n",
    "docs = pd.read_csv(DATA_DIR / 'dataset.csv')['description']\n",
    "topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('finance', 0.030844395493737967),\n",
       " ('imf', 0.02548989436190981),\n",
       " ('monetary', 0.021516056886781838),\n",
       " ('ministers', 0.017461659355147346),\n",
       " ('budget', 0.015147534599346599),\n",
       " ('imfworld', 0.01371422148963831),\n",
       " ('crisisracked', 0.01371422148963831),\n",
       " ('justpassed', 0.01371422148963831),\n",
       " ('asteppedup', 0.01371422148963831),\n",
       " ('gatheredunder', 0.01371422148963831)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similar_topics, similarity = topic_model.find_topics(\"finance\", top_n=5)\n",
    "topic_model.get_topic(similar_topics[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = np.array([SEEDS]).T\n",
    "seeds.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "41b1468c2a8aaa770bdb45a83028fbdb5ae0291ada001c7cfb7400daf117e2e6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('honours')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
