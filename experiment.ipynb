{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Getting Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer,BertForPreTraining, BertConfig\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "import re\n",
    "from scipy.spatial.distance import  cosine\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "DATA_DIR = Path('data')\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "SEEDS = ['finance', 'medicine', 'sports', 'technology']\n",
    "NUM_WORDS_PER_SET = 10\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def get_embeddings(model, tokens, embedding_size=768):\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "        embedding = output.last_hidden_state[0][1]\n",
    "        return torch.reshape(embedding, (embedding_size, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_batch(model, tokens, embedding_size=768, batch_size=4):\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "        embedding = output.last_hidden_state[:, 1, :]\n",
    "        return embedding\n",
    "\n",
    "\n",
    "def cos_distance_batch(topic, words):\n",
    "    return np.inner(\n",
    "        topic, words) / (np.linalg.norm(topic) * np.linalg.norm(words, axis=1))\n",
    "\n",
    "\n",
    "def job(vocab, topic, tokenizer, model, batch_size=4):\n",
    "    res_col = np.zeros((len(vocab), ))\n",
    "    vocab = list(vocab)\n",
    "    loop = tqdm(range(0, len(vocab), batch_size))\n",
    "    loop.set_description(f\"topic: {topic}\")\n",
    "    topic_token = tokenizer(topic,\n",
    "                            return_tensors='pt',\n",
    "                            padding=True,\n",
    "                            max_length=10,\n",
    "                            truncation=True)\n",
    "    topic_emb = get_embeddings(model, topic_token)\n",
    "    for batch_index in loop:\n",
    "        lo = batch_index\n",
    "        hi = min(batch_index + batch_size, len(vocab))\n",
    "        batch = vocab[batch_index:batch_index + batch_size]\n",
    "        tokens = tokenizer(batch,\n",
    "                           return_tensors='pt',\n",
    "                           padding='max_length',\n",
    "                           max_length=10,\n",
    "                           truncation=True)\n",
    "        # if len(token['input_ids']) > 3:\n",
    "        #     print(f\"WARNING: Word '{word}' is not in BERT's vocabulary\")\n",
    "        word_embs = get_embeddings_batch(model, tokens)\n",
    "        res_col[lo:hi] = cos_distance_batch(topic_emb, word_embs)\n",
    "        # res_col.append(cosine(topic_emb, word_emb))\n",
    "    return res_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 768])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = tokenizer(\"medicine\", return_tensors='pt', truncation=True)\n",
    "print(token.input_ids.shape)\n",
    "out = model(**token)\n",
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "tokens = {\n",
    "    \"medicine\": tokenizer(\"medicine\", return_tensors='pt', truncation=True),\n",
    "    \"cat\": tokenizer(\"cat\", return_tensors='pt', truncation=True),\n",
    "    \"kitty\": tokenizer('kitty', return_tensors='pt', truncation=True),\n",
    "    \"feline\": tokenizer('feline', return_tensors='pt', truncation=True),\n",
    "    \"doctor\": tokenizer(\"doctor\", return_tensors='pt', truncation=True),\n",
    "    \"medical\": tokenizer(\"medical\", return_tensors='pt', truncation=True),\n",
    "}\n",
    "embeddings = {}\n",
    "for key, val in tokens.items():\n",
    "    embeddings[key] = get_embeddings(model, val)\n",
    "\n",
    "print(\"cat-kitty: \" + f\"{cosine(embeddings['cat'], embeddings['kitty'])}\")\n",
    "print('cat-doctor: ' + f\"{cosine(embeddings['cat'], embeddings['doctor'])}\")\n",
    "print('medicine-cat: ' +\n",
    "      f\"{cosine(embeddings['medicine'], embeddings['cat'])}\")\n",
    "print('medicine-medical: ' +\n",
    "      f\"{cosine(embeddings['medicine'], embeddings['medical'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer('[MASK]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cat_emb = get_embeddings(cat_outputs)\n",
    "hello_emb = get_embeddings(hello_outputs)\n",
    "hi_emb = get_embeddings(hi_outputs)\n",
    "\n",
    "print(cosine(hi_emb, hello_emb))\n",
    "print(cosine(hi_emb, cat_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cosine([1], [0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Building vocab for the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                              title  \\\n",
       "0         3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1         3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2         3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3         3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4         3  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                         description  \n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...  \n",
       "1  Reuters - Private investment firm Carlyle Grou...  \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...  \n",
       "3  Reuters - Authorities have halted oil export\\f...  \n",
       "4  AFP - Tearaway world oil prices, toppling reco...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(Path('data') / 'dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [Errno 111] Connection\n",
      "[nltk_data]     refused>\n",
      "[nltk_data] Error loading wordnet: <urlopen error [Errno 111]\n",
      "[nltk_data]     Connection refused>\n",
      "[nltk_data] Error loading omw-1.4: <urlopen error [Errno 111]\n",
      "[nltk_data]     Connection refused>\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nltk.data.find('tokenizers/punkt.zip')\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords.zip')\n",
    "except:\n",
    "    nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def vocab_preprocess(row, lemmatize=True):\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    row = row.lower()\n",
    "    row = row.split('-')[1::]\n",
    "    row = ''.join(row)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # tokenize words\n",
    "    words = re.findall(re.compile('[a-zA-Z]+'), row)\n",
    "    # Remove stop words\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    if (lemmatize):\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_DIR / 'dataset.csv')\n",
    "desc = df['description'].astype(str)\n",
    "data = []\n",
    "for i, row in desc.iteritems():\n",
    "    data.append(vocab_preprocess(row, False))\n",
    "vocab = set()\n",
    "for words in data:\n",
    "    vocab = vocab.union(set(words))\n",
    "with open(DATA_DIR / \"vocab\" / \"global_vocab_no_lemmatize.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Build global rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50139\n"
     ]
    }
   ],
   "source": [
    "vocab = None\n",
    "with open(DATA_DIR / \"vocab\" / \"global_vocab_no_lemmatize.pkl\", \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'pathology' in vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab696714fac49a89166fd5a6af541a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/392 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cba4a04d8a74aef99a7ef066abf252c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/392 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ad1251c91764aa6be7cd4d4b0a1f8aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/392 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b27533f4631449528eba494a954650a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/392 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "topic_embeddings = []\n",
    "cols = []\n",
    "cos\n",
    "for seed in SEEDS:\n",
    "    cols.append(job(vocab, seed, tokenizer, model, batch_size=128))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50139,)\n",
      "(50139,)\n",
      "(50139,)\n",
      "(50139,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>finance</th>\n",
       "      <th>medicine</th>\n",
       "      <th>sports</th>\n",
       "      <th>technology</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_vocab</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>finance</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.723560</td>\n",
       "      <td>0.561143</td>\n",
       "      <td>0.688010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accounting</th>\n",
       "      <td>0.852869</td>\n",
       "      <td>0.753559</td>\n",
       "      <td>0.544790</td>\n",
       "      <td>0.710905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistics</th>\n",
       "      <td>0.841821</td>\n",
       "      <td>0.743688</td>\n",
       "      <td>0.547153</td>\n",
       "      <td>0.750695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marketing</th>\n",
       "      <td>0.836663</td>\n",
       "      <td>0.715334</td>\n",
       "      <td>0.589341</td>\n",
       "      <td>0.735878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>banking</th>\n",
       "      <td>0.831688</td>\n",
       "      <td>0.732591</td>\n",
       "      <td>0.513998</td>\n",
       "      <td>0.692174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>securities</th>\n",
       "      <td>0.802517</td>\n",
       "      <td>0.647551</td>\n",
       "      <td>0.520464</td>\n",
       "      <td>0.690203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>procurement</th>\n",
       "      <td>0.786893</td>\n",
       "      <td>0.691720</td>\n",
       "      <td>0.476832</td>\n",
       "      <td>0.664342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>financial</th>\n",
       "      <td>0.777519</td>\n",
       "      <td>0.647263</td>\n",
       "      <td>0.509762</td>\n",
       "      <td>0.705595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finances</th>\n",
       "      <td>0.777289</td>\n",
       "      <td>0.650105</td>\n",
       "      <td>0.544972</td>\n",
       "      <td>0.582143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>debt</th>\n",
       "      <td>0.773261</td>\n",
       "      <td>0.659070</td>\n",
       "      <td>0.459897</td>\n",
       "      <td>0.635410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              finance  medicine    sports  technology\n",
       "_vocab                                               \n",
       "finance      1.000000  0.723560  0.561143    0.688010\n",
       "accounting   0.852869  0.753559  0.544790    0.710905\n",
       "logistics    0.841821  0.743688  0.547153    0.750695\n",
       "marketing    0.836663  0.715334  0.589341    0.735878\n",
       "banking      0.831688  0.732591  0.513998    0.692174\n",
       "securities   0.802517  0.647551  0.520464    0.690203\n",
       "procurement  0.786893  0.691720  0.476832    0.664342\n",
       "financial    0.777519  0.647263  0.509762    0.705595\n",
       "finances     0.777289  0.650105  0.544972    0.582143\n",
       "debt         0.773261  0.659070  0.459897    0.635410"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array(cols)\n",
    "res_dict = {\n",
    "    '_vocab': list(vocab),\n",
    "}\n",
    "for i, topic in enumerate(SEEDS):\n",
    "    print(arr[i].shape)\n",
    "    res_dict[topic] = arr[i]\n",
    "res_df = pd.DataFrame(res_dict)\n",
    "res_df = res_df.set_index(['_vocab'])\n",
    "res_df.to_csv(DATA_DIR / 'global_cos_similarity.csv')\n",
    "res_df.sort_values('finance', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accounting logistics marketing\n",
      "pharmacy pathology medical\n",
      "baseball basketball athletics\n",
      "engineering robotics telecommunication\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./global_cos_similarity.csv', index_col='_vocab')\n",
    "for seed in SEEDS:\n",
    "    line = df.sort_values(seed, ascending=False).head(4).index[1::]\n",
    "    print(\" \".join(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>finance</th>\n",
       "      <th>medicine</th>\n",
       "      <th>sports</th>\n",
       "      <th>technology</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_vocab</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>finance</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.723560</td>\n",
       "      <td>0.561142</td>\n",
       "      <td>0.688010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accounting</th>\n",
       "      <td>0.852870</td>\n",
       "      <td>0.753559</td>\n",
       "      <td>0.544790</td>\n",
       "      <td>0.710905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistics</th>\n",
       "      <td>0.841821</td>\n",
       "      <td>0.743687</td>\n",
       "      <td>0.547152</td>\n",
       "      <td>0.750695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marketing</th>\n",
       "      <td>0.836664</td>\n",
       "      <td>0.715334</td>\n",
       "      <td>0.589341</td>\n",
       "      <td>0.735878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>banking</th>\n",
       "      <td>0.831688</td>\n",
       "      <td>0.732591</td>\n",
       "      <td>0.513997</td>\n",
       "      <td>0.692174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>procurement</th>\n",
       "      <td>0.786893</td>\n",
       "      <td>0.691719</td>\n",
       "      <td>0.476832</td>\n",
       "      <td>0.664342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>financial</th>\n",
       "      <td>0.777519</td>\n",
       "      <td>0.647263</td>\n",
       "      <td>0.509762</td>\n",
       "      <td>0.705595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>debt</th>\n",
       "      <td>0.773262</td>\n",
       "      <td>0.659070</td>\n",
       "      <td>0.459897</td>\n",
       "      <td>0.635410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>engineering</th>\n",
       "      <td>0.773229</td>\n",
       "      <td>0.726308</td>\n",
       "      <td>0.627797</td>\n",
       "      <td>0.805677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advertising</th>\n",
       "      <td>0.769661</td>\n",
       "      <td>0.681448</td>\n",
       "      <td>0.512919</td>\n",
       "      <td>0.688041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              finance  medicine    sports  technology\n",
       "_vocab                                               \n",
       "finance      1.000000  0.723560  0.561142    0.688010\n",
       "accounting   0.852870  0.753559  0.544790    0.710905\n",
       "logistics    0.841821  0.743687  0.547152    0.750695\n",
       "marketing    0.836664  0.715334  0.589341    0.735878\n",
       "banking      0.831688  0.732591  0.513997    0.692174\n",
       "procurement  0.786893  0.691719  0.476832    0.664342\n",
       "financial    0.777519  0.647263  0.509762    0.705595\n",
       "debt         0.773262  0.659070  0.459897    0.635410\n",
       "engineering  0.773229  0.726308  0.627797    0.805677\n",
       "advertising  0.769661  0.681448  0.512919    0.688041"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.sort_values(by='finance', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Initialize word sets from $e$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>finance</th>\n",
       "      <th>medicine</th>\n",
       "      <th>sports</th>\n",
       "      <th>technology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>finance</td>\n",
       "      <td>medicine</td>\n",
       "      <td>sport</td>\n",
       "      <td>technology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>accounting</td>\n",
       "      <td>pharmacy</td>\n",
       "      <td>baseball</td>\n",
       "      <td>robotics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>logistics</td>\n",
       "      <td>pathology</td>\n",
       "      <td>basketball</td>\n",
       "      <td>telecommunication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>marketing</td>\n",
       "      <td>medical</td>\n",
       "      <td>athletics</td>\n",
       "      <td>communication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>banking</td>\n",
       "      <td>biology</td>\n",
       "      <td>gymnastics</td>\n",
       "      <td>journalism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>procurement</td>\n",
       "      <td>surgery</td>\n",
       "      <td>volleyball</td>\n",
       "      <td>philosophy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>financial</td>\n",
       "      <td>health</td>\n",
       "      <td>football</td>\n",
       "      <td>industrial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>debt</td>\n",
       "      <td>astronomy</td>\n",
       "      <td>swimming</td>\n",
       "      <td>drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>engineering</td>\n",
       "      <td>industry</td>\n",
       "      <td>hockey</td>\n",
       "      <td>integration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>advertising</td>\n",
       "      <td>nutrition</td>\n",
       "      <td>education</td>\n",
       "      <td>sociology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>management</td>\n",
       "      <td>architecture</td>\n",
       "      <td>broadcasting</td>\n",
       "      <td>culture</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        finance      medicine        sports         technology\n",
       "0       finance      medicine         sport         technology\n",
       "1    accounting      pharmacy      baseball           robotics\n",
       "2     logistics     pathology    basketball  telecommunication\n",
       "3     marketing       medical     athletics      communication\n",
       "4       banking       biology    gymnastics         journalism\n",
       "5   procurement       surgery    volleyball         philosophy\n",
       "6     financial        health      football         industrial\n",
       "7          debt     astronomy      swimming              drama\n",
       "8   engineering      industry        hockey        integration\n",
       "9   advertising     nutrition     education          sociology\n",
       "10   management  architecture  broadcasting            culture"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "res_df = pd.read_csv('global_cos_similarity.csv')\n",
    "res_df = res_df.set_index(['_vocab'])\n",
    "word_set = {}\n",
    "added_words = set()\n",
    "for seed in SEEDS:\n",
    "    word_set[seed] = []\n",
    "    col = res_df[seed]\n",
    "    sorted_col = col.sort_values()[::-1]\n",
    "    i = 0\n",
    "    for word, _ in sorted_col.iteritems():\n",
    "        if i == NUM_WORDS_PER_SET + 1:\n",
    "            break\n",
    "        if word not in added_words:\n",
    "            word_set[seed].append(word)\n",
    "            added_words.add(word)\n",
    "            i += 1\n",
    "\n",
    "pd.DataFrame(word_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accounting logistics marketing banking procurement financial debt engineering advertising management\n",
      "pharmacy pathology medical biology surgery health accounting astronomy logistics industry\n",
      "baseball basketball athletics gymnastics volleyball football swimming hockey education broadcasting\n",
      "engineering robotics telecommunication communication logistics journalism industry philosophy industrial marketing\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Local Knowlege using pretrained BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/home/zhang/.cache/huggingface/datasets/ag_news/default/0.0.0/bc2bcb40336ace1a0374767fc29bb0296cdaf8a6da7298436239c54d79180548)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aa8c6e5626f41fd8cdedb1a9b55afeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare datasets\n",
    "from transformers import BertTokenizer, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15,\n",
    ")\n",
    "dataset = load_dataset('ag_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 127600\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenate_datasets([dataset['train'], dataset['test']], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python3 scripts/train.py \\\n",
    "    -o ./models \\\n",
    "    -t ./data/tokens/tokens-pretrained-30522.pkl \\\n",
    "    -n bert-pretrined-30522 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Compute Local Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at models/bert-pretrained-pretrained were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('models/bert-pretrained-pretrained')\n",
    "model.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4,)\n",
      "[0.863026016463138, 0.8683996327753427, 0.8716901238866911, 0.8647266556320791]\n"
     ]
    }
   ],
   "source": [
    "a = np.ones((768, ))\n",
    "b = np.random.rand(4, 768)\n",
    "np_cos = np.inner(a, b) / (np.linalg.norm(a) * np.linalg.norm(b, axis=1))\n",
    "scp_cos = [1-cosine(a, b[i]) for i in range(4)]\n",
    "print(np_cos.shape)\n",
    "print(scp_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_embeddings_batch(model, tokens, embedding_size=768, batch_size=4):\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "        embedding = output.last_hidden_state[:, 1, :]\n",
    "        return embedding\n",
    "\n",
    "\n",
    "def cos_distance_batch(topic, words):\n",
    "    return np.inner(topic, words) / (np.linalg.norm(topic) * np.linalg.norm(words, axis=1))\n",
    "\n",
    "\n",
    "def job(vocab, topic, tokenizer, model, batch_size=4):\n",
    "    res_col = np.zeros((len(vocab), ))\n",
    "    vocab = list(vocab)\n",
    "    loop = tqdm(range(0, len(vocab), batch_size))\n",
    "    loop.set_description(f\"topic: {topic}\")\n",
    "    topic_token = tokenizer(topic, return_tensors='pt', padding=True, max_length=10, truncation=True)\n",
    "    topic_emb = get_embeddings(model, topic_token)\n",
    "    for batch_index in loop:\n",
    "        lo = batch_index\n",
    "        hi = min(batch_index + batch_size, len(vocab))\n",
    "        batch = vocab[batch_index:batch_index + batch_size]\n",
    "        tokens = tokenizer(batch, return_tensors='pt', padding='max_length', max_length=10, truncation=True)\n",
    "        # if len(token['input_ids']) > 3:\n",
    "        #     print(f\"WARNING: Word '{word}' is not in BERT's vocabulary\")\n",
    "        word_embs = get_embeddings_batch(model, tokens)\n",
    "        res_col[lo:hi] = cos_distance_batch(topic_emb, word_embs)\n",
    "        # res_col.append(cosine(topic_emb, word_emb))\n",
    "    return res_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063de2b81a194510ad07d802ab8c0edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8525cf83ee4b4aedbff36952771af4cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc43d3dd6d5b45dfb57d81be5dc32367",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1416610c0f4925b1bb1b5a429f0016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/178 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for topic in SEEDS:\n",
    "    res_col = job(vocab, topic, tokenizer, model, batch_size=256)\n",
    "    res.append(res_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "arr = np.array(res)\n",
    "arr = arr.T\n",
    "res_dict = {    \n",
    "    '_vocab': list(vocab),\n",
    "}\n",
    "for i, topic in enumerate(SEEDS):\n",
    "    res_dict[topic] = arr[:, i]\n",
    "res_df = pd.DataFrame(res_dict)\n",
    "res_df = res_df.set_index(['_vocab'])\n",
    "res_df.to_csv('local_embeddings_bert-pretrained-pretrained.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>finance</th>\n",
       "      <th>medicine</th>\n",
       "      <th>sports</th>\n",
       "      <th>technology</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_vocab</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>finance</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.676248</td>\n",
       "      <td>0.507404</td>\n",
       "      <td>0.631834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>financial</th>\n",
       "      <td>0.790944</td>\n",
       "      <td>0.699580</td>\n",
       "      <td>0.524704</td>\n",
       "      <td>0.627311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>financed</th>\n",
       "      <td>0.783384</td>\n",
       "      <td>0.739271</td>\n",
       "      <td>0.466230</td>\n",
       "      <td>0.727619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>engineering</th>\n",
       "      <td>0.774250</td>\n",
       "      <td>0.791155</td>\n",
       "      <td>0.548729</td>\n",
       "      <td>0.719049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>banking</th>\n",
       "      <td>0.768823</td>\n",
       "      <td>0.696595</td>\n",
       "      <td>0.548499</td>\n",
       "      <td>0.640492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>corporate</th>\n",
       "      <td>0.766297</td>\n",
       "      <td>0.744483</td>\n",
       "      <td>0.532844</td>\n",
       "      <td>0.662084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vice</th>\n",
       "      <td>0.763028</td>\n",
       "      <td>0.741325</td>\n",
       "      <td>0.574542</td>\n",
       "      <td>0.577830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>catering</th>\n",
       "      <td>0.758489</td>\n",
       "      <td>0.780816</td>\n",
       "      <td>0.556359</td>\n",
       "      <td>0.696496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>investing</th>\n",
       "      <td>0.757830</td>\n",
       "      <td>0.702346</td>\n",
       "      <td>0.531790</td>\n",
       "      <td>0.597072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accounting</th>\n",
       "      <td>0.750611</td>\n",
       "      <td>0.714651</td>\n",
       "      <td>0.515481</td>\n",
       "      <td>0.623839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              finance  medicine    sports  technology\n",
       "_vocab                                               \n",
       "finance      1.000000  0.676248  0.507404    0.631834\n",
       "financial    0.790944  0.699580  0.524704    0.627311\n",
       "financed     0.783384  0.739271  0.466230    0.727619\n",
       "engineering  0.774250  0.791155  0.548729    0.719049\n",
       "banking      0.768823  0.696595  0.548499    0.640492\n",
       "corporate    0.766297  0.744483  0.532844    0.662084\n",
       "vice         0.763028  0.741325  0.574542    0.577830\n",
       "catering     0.758489  0.780816  0.556359    0.696496\n",
       "investing    0.757830  0.702346  0.531790    0.597072\n",
       "accounting   0.750611  0.714651  0.515481    0.623839"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df = pd.read_csv('./local_embeddings_bert-pretrained-pretrained.csv', index_col='_vocab')\n",
    "res_df.sort_values('finance', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "seeds = np.array([SEEDS]).T.tolist()\n",
    "topic_model = BERTopic(seed_topic_list=seeds)\n",
    "docs = pd.read_csv(DATA_DIR / 'dataset.csv')['description']\n",
    "topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monetary 0.7938800681394652\n",
      "funds 0.7234529732634416\n",
      "securities 0.7051542937552662\n",
      "freddie 0.6877536466448553\n",
      "poorest 0.6766799869765071\n"
     ]
    }
   ],
   "source": [
    "similar_topics, similarity = topic_model.find_topics(\"finance\", top_n=5)\n",
    "for i, topic in enumerate(similar_topics):\n",
    "    print(topic_model.get_topic(topic)[0][0], similarity[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Cate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "41b1468c2a8aaa770bdb45a83028fbdb5ae0291ada001c7cfb7400daf117e2e6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('honours')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
