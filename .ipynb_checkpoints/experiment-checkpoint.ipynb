{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Getting Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 111]\n",
      "[nltk_data]     Connection refused>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel, BertTokenizer,BertForPreTraining, BertConfig\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import nltk\n",
    "from pathlib import Path\n",
    "import re\n",
    "from scipy.spatial.distance import  cosine\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import Parallel, delayed\n",
    "DATA_DIR = Path('data')\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "SEEDS = ['finance', 'medicine', 'sports', 'technology']\n",
    "NUM_WORDS_PER_SET = 10\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "def get_embeddings(model, tokens, embedding_size=768):\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "        embedding = output.last_hidden_state[0][1]\n",
    "        return torch.reshape(embedding, (embedding_size, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_embeddings_batch(model, tokens, embedding_size=768, batch_size=4):\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "        embedding = output.last_hidden_state[:, 1, :]\n",
    "        return embedding\n",
    "\n",
    "\n",
    "def cos_distance_batch(topic, words):\n",
    "    return np.inner(\n",
    "        topic, words) / (np.linalg.norm(topic) * np.linalg.norm(words, axis=1))\n",
    "\n",
    "\n",
    "def job(vocab, topic, tokenizer, model, batch_size=4):\n",
    "    res_col = np.zeros((len(vocab), ))\n",
    "    vocab = list(vocab)\n",
    "    loop = tqdm(range(0, len(vocab), batch_size))\n",
    "    loop.set_description(f\"topic: {topic}\")\n",
    "    topic_token = tokenizer(topic,\n",
    "                            return_tensors='pt',\n",
    "                            padding=True,\n",
    "                            max_length=10,\n",
    "                            truncation=True)\n",
    "    topic_emb = get_embeddings(model, topic_token)\n",
    "    for batch_index in loop:\n",
    "        lo = batch_index\n",
    "        hi = min(batch_index + batch_size, len(vocab))\n",
    "        batch = vocab[batch_index:batch_index + batch_size]\n",
    "        tokens = tokenizer(batch,\n",
    "                           return_tensors='pt',\n",
    "                           padding='max_length',\n",
    "                           max_length=10,\n",
    "                           truncation=True)\n",
    "        # if len(token['input_ids']) > 3:\n",
    "        #     print(f\"WARNING: Word '{word}' is not in BERT's vocabulary\")\n",
    "        word_embs = get_embeddings_batch(model, tokens)\n",
    "        res_col[lo:hi] = cos_distance_batch(topic_emb, word_embs)\n",
    "        # res_col.append(cosine(topic_emb, word_emb))\n",
    "    return res_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "token = tokenizer(\"medicine\", return_tensors='pt', truncation=True)\n",
    "print(token.input_ids.shape)\n",
    "out = model(**token)\n",
    "out.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer('[MASK]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cat_emb = get_embeddings(cat_outputs)\n",
    "hello_emb = get_embeddings(hello_outputs)\n",
    "hi_emb = get_embeddings(hi_outputs)\n",
    "\n",
    "print(cosine(hi_emb, hello_emb))\n",
    "print(cosine(hi_emb, cat_emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cosine([1], [0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Building vocab for the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(Path('data') / 'content_cleaned.txt')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def vocab_preprocess(row, lemmatize=True):\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    row = row.lower().strip()\n",
    "    words = row.split(' ')\n",
    "    # Remove stop words\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    res = {}\n",
    "    for word in words:\n",
    "        count = res.get(word, 0)\n",
    "        count += 1\n",
    "        res[word] = count\n",
    "    if (lemmatize):\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7977a9aa577449794376940dc3e32d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eedf5882a9e45a18a114fe7c3f4da71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48223 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22346\n",
      "48223\n",
      "25877\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT=3\n",
    "f = open('data/content_cleaned.txt')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "data = []\n",
    "vocab = {}\n",
    "for i, row in tqdm(enumerate(lines)):\n",
    "    freq = vocab_preprocess(row, False)\n",
    "    for _word, _count in freq.items():\n",
    "        count = vocab.get(_word, 0) + _count\n",
    "        vocab[_word] = count\n",
    "uncommon_words = []\n",
    "for word, count in tqdm(vocab.items()):\n",
    "    if count < MIN_COUNT:\n",
    "       uncommon_words.append(word)\n",
    "print(len(uncommon_words))\n",
    "print(len(vocab.keys()))\n",
    "for word in uncommon_words:\n",
    "    del vocab[word]\n",
    "print(len(vocab.keys()))\n",
    "with open(DATA_DIR / \"vocab\" / \"global_vocab_no_lemmatize.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Build global rankings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25877\n"
     ]
    }
   ],
   "source": [
    "vocab = None\n",
    "with open(DATA_DIR / \"vocab\" / \"global_vocab_no_lemmatize.pkl\", \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9643367d4a544fa809dc284b44493d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42812c4e5b724d6083ad372e853f69c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6033b2efe5e943faada9d3e2f5d787ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fec7ba8d6e3461a848c499d079413e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/203 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "topic_embeddings = []\n",
    "cols = []\n",
    "for seed in SEEDS:\n",
    "    cols.append(job(vocab, seed, tokenizer, model, batch_size=128))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25877,)\n",
      "(25877,)\n",
      "(25877,)\n",
      "(25877,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>finance</th>\n",
       "      <th>medicine</th>\n",
       "      <th>sports</th>\n",
       "      <th>technology</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_vocab</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>finance</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.723560</td>\n",
       "      <td>0.561143</td>\n",
       "      <td>0.688010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accounting</th>\n",
       "      <td>0.852869</td>\n",
       "      <td>0.753559</td>\n",
       "      <td>0.544790</td>\n",
       "      <td>0.710905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistics</th>\n",
       "      <td>0.841821</td>\n",
       "      <td>0.743688</td>\n",
       "      <td>0.547153</td>\n",
       "      <td>0.750695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marketing</th>\n",
       "      <td>0.836663</td>\n",
       "      <td>0.715334</td>\n",
       "      <td>0.589341</td>\n",
       "      <td>0.735878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>banking</th>\n",
       "      <td>0.831688</td>\n",
       "      <td>0.732591</td>\n",
       "      <td>0.513998</td>\n",
       "      <td>0.692174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>securities</th>\n",
       "      <td>0.802517</td>\n",
       "      <td>0.647551</td>\n",
       "      <td>0.520464</td>\n",
       "      <td>0.690203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>procurement</th>\n",
       "      <td>0.786893</td>\n",
       "      <td>0.691720</td>\n",
       "      <td>0.476832</td>\n",
       "      <td>0.664342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>financial</th>\n",
       "      <td>0.777519</td>\n",
       "      <td>0.647263</td>\n",
       "      <td>0.509762</td>\n",
       "      <td>0.705595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finances</th>\n",
       "      <td>0.777289</td>\n",
       "      <td>0.650105</td>\n",
       "      <td>0.544972</td>\n",
       "      <td>0.582143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>debt</th>\n",
       "      <td>0.773261</td>\n",
       "      <td>0.659070</td>\n",
       "      <td>0.459897</td>\n",
       "      <td>0.635410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              finance  medicine    sports  technology\n",
       "_vocab                                               \n",
       "finance      1.000000  0.723560  0.561143    0.688010\n",
       "accounting   0.852869  0.753559  0.544790    0.710905\n",
       "logistics    0.841821  0.743688  0.547153    0.750695\n",
       "marketing    0.836663  0.715334  0.589341    0.735878\n",
       "banking      0.831688  0.732591  0.513998    0.692174\n",
       "securities   0.802517  0.647551  0.520464    0.690203\n",
       "procurement  0.786893  0.691720  0.476832    0.664342\n",
       "financial    0.777519  0.647263  0.509762    0.705595\n",
       "finances     0.777289  0.650105  0.544972    0.582143\n",
       "debt         0.773261  0.659070  0.459897    0.635410"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = np.array(cols)\n",
    "res_dict = {\n",
    "    '_vocab': list(vocab),\n",
    "}\n",
    "for i, topic in enumerate(SEEDS):\n",
    "    print(arr[i].shape)\n",
    "    res_dict[topic] = arr[i]\n",
    "res_df = pd.DataFrame(res_dict)\n",
    "res_df = res_df.set_index(['_vocab'])\n",
    "res_df.to_csv('./results/global_cos_similarity.csv')\n",
    "res_df.sort_values('finance', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accounting logistics marketing\n",
      "dentistry pharmacy pathology\n",
      "sport baseball basketball\n",
      "technologies engineering robotics\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./results/global_cos_similarity.csv', index_col='_vocab')\n",
    "for seed in SEEDS:\n",
    "    line = df.sort_values(seed, ascending=False).head(4).index[1::]\n",
    "    print(\" \".join(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>finance</th>\n",
       "      <th>medicine</th>\n",
       "      <th>sports</th>\n",
       "      <th>technology</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_vocab</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>technology</th>\n",
       "      <td>0.688011</td>\n",
       "      <td>0.640761</td>\n",
       "      <td>0.592079</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>technologies</th>\n",
       "      <td>0.685335</td>\n",
       "      <td>0.582082</td>\n",
       "      <td>0.502387</td>\n",
       "      <td>0.819644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>engineering</th>\n",
       "      <td>0.773229</td>\n",
       "      <td>0.726307</td>\n",
       "      <td>0.627797</td>\n",
       "      <td>0.805677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>robotics</th>\n",
       "      <td>0.702049</td>\n",
       "      <td>0.674241</td>\n",
       "      <td>0.586133</td>\n",
       "      <td>0.802258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>telecommunications</th>\n",
       "      <td>0.703807</td>\n",
       "      <td>0.684840</td>\n",
       "      <td>0.600680</td>\n",
       "      <td>0.784153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>systems</th>\n",
       "      <td>0.673512</td>\n",
       "      <td>0.610776</td>\n",
       "      <td>0.539074</td>\n",
       "      <td>0.770397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>telecommunication</th>\n",
       "      <td>0.686691</td>\n",
       "      <td>0.670584</td>\n",
       "      <td>0.500605</td>\n",
       "      <td>0.756258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>communication</th>\n",
       "      <td>0.755890</td>\n",
       "      <td>0.723222</td>\n",
       "      <td>0.544936</td>\n",
       "      <td>0.756241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistics</th>\n",
       "      <td>0.841821</td>\n",
       "      <td>0.743688</td>\n",
       "      <td>0.547153</td>\n",
       "      <td>0.750695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>journalism</th>\n",
       "      <td>0.641313</td>\n",
       "      <td>0.656924</td>\n",
       "      <td>0.595752</td>\n",
       "      <td>0.748470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     finance  medicine    sports  technology\n",
       "_vocab                                                      \n",
       "technology          0.688011  0.640761  0.592079    1.000000\n",
       "technologies        0.685335  0.582082  0.502387    0.819644\n",
       "engineering         0.773229  0.726307  0.627797    0.805677\n",
       "robotics            0.702049  0.674241  0.586133    0.802258\n",
       "telecommunications  0.703807  0.684840  0.600680    0.784153\n",
       "systems             0.673512  0.610776  0.539074    0.770397\n",
       "telecommunication   0.686691  0.670584  0.500605    0.756258\n",
       "communication       0.755890  0.723222  0.544936    0.756241\n",
       "logistics           0.841821  0.743688  0.547153    0.750695\n",
       "journalism          0.641313  0.656924  0.595752    0.748470"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df.sort_values(by='technology', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Initialize word sets from $e$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "res_df = pd.read_csv('global_cos_similarity.csv')\n",
    "res_df = res_df.set_index(['_vocab'])\n",
    "word_set = {}\n",
    "added_words = set()\n",
    "for seed in SEEDS:\n",
    "    word_set[seed] = []\n",
    "    col = res_df[seed]\n",
    "    sorted_col = col.sort_values()[::-1]\n",
    "    i = 0\n",
    "    for word, _ in sorted_col.iteritems():\n",
    "        if i == NUM_WORDS_PER_SET + 1:\n",
    "            break\n",
    "        if word not in added_words:\n",
    "            word_set[seed].append(word)\n",
    "            added_words.add(word)\n",
    "            i += 1\n",
    "\n",
    "word_set_df = pd.DataFrame(word_set)\n",
    "word_set_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accounting logistics marketing\n",
      "pharmacy pathology medical\n",
      "baseball basketball athletics\n",
      "robotics telecommunication communication\n"
     ]
    }
   ],
   "source": [
    "for topic, words in word_set.items():\n",
    "    print(f\"{' '.join(words[1:4])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Local Knowlege using pretrained BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare datasets\n",
    "from transformers import BertTokenizer, DataCollatorForLanguageModeling\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15,\n",
    ")\n",
    "dataset = load_dataset('ag_news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "concatenate_datasets([dataset['train'], dataset['test']], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "python3 scripts/train.py \\\n",
    "    -o ./models \\\n",
    "    -t ./data/tokens/tokens-pretrained-30522.pkl \\\n",
    "    -n bert-pretrined-30522 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Compute Local Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = BertModel.from_pretrained('models/bert-pretrained-pretrained')\n",
    "model.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "a = np.ones((768, ))\n",
    "b = np.random.rand(4, 768)\n",
    "np_cos = np.inner(a, b) / (np.linalg.norm(a) * np.linalg.norm(b, axis=1))\n",
    "scp_cos = [1-cosine(a, b[i]) for i in range(4)]\n",
    "print(np_cos.shape)\n",
    "print(scp_cos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_embeddings_batch(model, tokens, embedding_size=768, batch_size=4):\n",
    "    with torch.no_grad():\n",
    "        output = model(**tokens)\n",
    "        embedding = output.last_hidden_state[:, 1, :]\n",
    "        return embedding\n",
    "\n",
    "\n",
    "def cos_distance_batch(topic, words):\n",
    "    return np.inner(topic, words) / (np.linalg.norm(topic) * np.linalg.norm(words, axis=1))\n",
    "\n",
    "\n",
    "def job(vocab, topic, tokenizer, model, batch_size=4):\n",
    "    res_col = np.zeros((len(vocab), ))\n",
    "    vocab = list(vocab)\n",
    "    loop = tqdm(range(0, len(vocab), batch_size))\n",
    "    loop.set_description(f\"topic: {topic}\")\n",
    "    topic_token = tokenizer(topic, return_tensors='pt', padding=True, max_length=10, truncation=True)\n",
    "    topic_emb = get_embeddings(model, topic_token)\n",
    "    for batch_index in loop:\n",
    "        lo = batch_index\n",
    "        hi = min(batch_index + batch_size, len(vocab))\n",
    "        batch = vocab[batch_index:batch_index + batch_size]\n",
    "        tokens = tokenizer(batch, return_tensors='pt', padding='max_length', max_length=10, truncation=True)\n",
    "        # if len(token['input_ids']) > 3:\n",
    "        #     print(f\"WARNING: Word '{word}' is not in BERT's vocabulary\")\n",
    "        word_embs = get_embeddings_batch(model, tokens)\n",
    "        res_col[lo:hi] = cos_distance_batch(topic_emb, word_embs)\n",
    "        # res_col.append(cosine(topic_emb, word_emb))\n",
    "    return res_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2571978e234b30a2413af8c23804ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d95ad3aa2244029a7e1d4ff4b1ced5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "180c910a0ecb457b9bd357ca24782605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651d49a99b9a439a886e4ec0bcf17322",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/94 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res = []\n",
    "for topic in SEEDS:\n",
    "    res_col = job(vocab, topic, tokenizer, model, batch_size=256)\n",
    "    res.append(res_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "arr = np.array(res)\n",
    "arr = arr.T\n",
    "res_dict = {    \n",
    "    '_vocab': list(vocab),\n",
    "}\n",
    "for i, topic in enumerate(SEEDS):\n",
    "    res_dict[topic] = arr[:, i]\n",
    "res_df = pd.DataFrame(res_dict)\n",
    "res_df = res_df.set_index(['_vocab'])\n",
    "res_df.to_csv('local_embeddings_bert-pretrained-pretrained.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>finance</th>\n",
       "      <th>medicine</th>\n",
       "      <th>sports</th>\n",
       "      <th>technology</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_vocab</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>finance</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.723560</td>\n",
       "      <td>0.561143</td>\n",
       "      <td>0.688010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accounting</th>\n",
       "      <td>0.852869</td>\n",
       "      <td>0.753559</td>\n",
       "      <td>0.544790</td>\n",
       "      <td>0.710905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistics</th>\n",
       "      <td>0.841821</td>\n",
       "      <td>0.743688</td>\n",
       "      <td>0.547153</td>\n",
       "      <td>0.750695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>marketing</th>\n",
       "      <td>0.836663</td>\n",
       "      <td>0.715334</td>\n",
       "      <td>0.589341</td>\n",
       "      <td>0.735878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>banking</th>\n",
       "      <td>0.831688</td>\n",
       "      <td>0.732591</td>\n",
       "      <td>0.513998</td>\n",
       "      <td>0.692174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>securities</th>\n",
       "      <td>0.802517</td>\n",
       "      <td>0.647551</td>\n",
       "      <td>0.520464</td>\n",
       "      <td>0.690203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>procurement</th>\n",
       "      <td>0.786893</td>\n",
       "      <td>0.691720</td>\n",
       "      <td>0.476832</td>\n",
       "      <td>0.664342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>financial</th>\n",
       "      <td>0.777519</td>\n",
       "      <td>0.647263</td>\n",
       "      <td>0.509762</td>\n",
       "      <td>0.705595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>finances</th>\n",
       "      <td>0.777289</td>\n",
       "      <td>0.650105</td>\n",
       "      <td>0.544972</td>\n",
       "      <td>0.582143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>debt</th>\n",
       "      <td>0.773261</td>\n",
       "      <td>0.659070</td>\n",
       "      <td>0.459897</td>\n",
       "      <td>0.635410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              finance  medicine    sports  technology\n",
       "_vocab                                               \n",
       "finance      1.000000  0.723560  0.561143    0.688010\n",
       "accounting   0.852869  0.753559  0.544790    0.710905\n",
       "logistics    0.841821  0.743688  0.547153    0.750695\n",
       "marketing    0.836663  0.715334  0.589341    0.735878\n",
       "banking      0.831688  0.732591  0.513998    0.692174\n",
       "securities   0.802517  0.647551  0.520464    0.690203\n",
       "procurement  0.786893  0.691720  0.476832    0.664342\n",
       "financial    0.777519  0.647263  0.509762    0.705595\n",
       "finances     0.777289  0.650105  0.544972    0.582143\n",
       "debt         0.773261  0.659070  0.459897    0.635410"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df = pd.read_csv('./local_embeddings_bert-pretrained-pretrained.csv', index_col='_vocab')\n",
    "res_df.sort_values('finance', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "seeds = np.array([SEEDS]).T.tolist()\n",
    "topic_model = BERTopic(seed_topic_list=seeds)\n",
    "docs = pd.read_csv(DATA_DIR / 'dataset.csv')['description']\n",
    "topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monetary 0.791343401281863\n",
      "finance 0.7557908228754602\n",
      "funds 0.7344708126048249\n",
      "freddie 0.6878446711889306\n",
      "securities 0.6788982583268262\n"
     ]
    }
   ],
   "source": [
    "similar_topics, similarity = topic_model.find_topics(\"finance\", top_n=5)\n",
    "for i, topic in enumerate(similar_topics):\n",
    "    print(topic_model.get_topic(topic)[0][0], similarity[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# CatE\n",
    "**kwown issue**: Seg Fault when seed words > 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "local_emb_vocab_df = pd.read_csv('./results/CatE/emb_seeds_w.txt', sep=' ', skiprows=[0], index_col=0, header=None)\n",
    "local_emb_vocab_df.dropna(axis=1, inplace=True)\n",
    "\n",
    "local_emb_topic_df = pd.read_csv('./results/CatE/emb_seeds_t.txt', sep=' ', skiprows=[0], index_col=0, header=None)\n",
    "local_emb_topic_df.dropna(axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def compute_cate_cos_similarity(vocab, vocab_embs, topic_emb, topic, batch_size=32, show_progress=True):\n",
    "    res_col = np.zeros((len(vocab), ))\n",
    "    loop = range(0, len(vocab), batch_size)\n",
    "    if show_progress:\n",
    "        loop = tqdm(loop)\n",
    "        loop.set_description(f\"topic: {topic}\")\n",
    "    for batch_index in loop:\n",
    "        lo = batch_index\n",
    "        hi = min(batch_index + batch_size, len(vocab))\n",
    "\n",
    "        batch = vocab[lo:hi]\n",
    "        batch_embs = vocab_embs.loc[batch, :]\n",
    "        res_col[lo:hi] = cos_distance_batch(topic_emb, batch_embs)\n",
    "        # res_col.append(cosine(topic_emb, word_emb))\n",
    "    return res_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "vocab_f = open('./data/vocab/global_vocab_no_lemmatize.pkl', 'rb')\n",
    "vocab = pickle.load(vocab_f)\n",
    "vocab_f.close()\n",
    "not_in_vocab_words = []\n",
    "for word in vocab.keys():\n",
    "    if word not in local_emb_vocab_df.index:\n",
    "        not_in_vocab_words.append(word)\n",
    "print(len(not_in_vocab_words))\n",
    "for word in not_in_vocab_words:\n",
    "    del vocab[word]\n",
    "for word in local_emb_vocab_df.index:\n",
    "    if word not in vocab:\n",
    "        local_emb_vocab_df =  local_emb_vocab_df.drop(word, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "res = {}\n",
    "res['_vocab'] = vocab.keys()\n",
    "for topic in local_emb_topic_df.index:\n",
    "    topic_emb = local_emb_topic_df.loc[topic, :]\n",
    "    res_col = compute_cate_cos_similarity(list(vocab.keys()), local_emb_vocab_df, topic_emb, topic, show_progress=False)\n",
    "    res[topic] = res_col\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(res)\n",
    "res_df = res_df.set_index(['_vocab'])\n",
    "res_df.to_csv('./results/cate_local_embeddings.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Compute Ensemble ranking [Proposed]\n",
    "Which does not work as expected -> CatE has different embeddings for the same word in topic and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "local_emb_vocab_df = pd.read_csv('./results/CatE/emb_seeds_w.txt', sep=' ', skiprows=[0], index_col=0, header=None)\n",
    "local_emb_vocab_df.dropna(axis=1, inplace=True)\n",
    "\n",
    "local_emb_topic_df = pd.read_csv('./results/CatE/emb_seeds_t.txt', sep=' ', skiprows=[0], index_col=0, header=None)\n",
    "local_emb_topic_df.dropna(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faace728ad9d41199d3d786c18a24e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25876 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_vocab_cos_similarity(emb):\n",
    "    out_shape = (emb.shape[0], emb.shape[0])\n",
    "    df = pd.DataFrame(np.zeros((out_shape)), index=emb.index, columns=emb.index)\n",
    "    for word in tqdm(emb.index):\n",
    "        topic_emb = emb.loc[word, :]\n",
    "        res_row = cos_distance_batch(topic_emb, emb)\n",
    "        df.loc[word, :] = res_row\n",
    "    return df\n",
    "res = compute_vocab_cos_similarity(local_emb_vocab_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5662511389486806"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.loc['accounting', 'unumprovident']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33416083167506283"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine(local_emb_vocab_df.loc['accounting', :], local_emb_topic_df.loc['accounting', :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Compute emsemble ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "global_cos_df = pd.read_csv('results/global_cos_similarity.csv', index_col=0)\n",
    "local_cos_df = pd.read_csv('./results/cate_local_embeddings.csv', index_col=0)\n",
    "# min_max_scalar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_vocab\n",
       "accounting    0.880459\n",
       "marketing     0.843212\n",
       "logistics     0.842892\n",
       "financial     0.799560\n",
       "securities    0.789042\n",
       "                ...   \n",
       "oxley         0.028208\n",
       "forgotten     0.027069\n",
       "morten        0.016974\n",
       "winky         0.000000\n",
       "horns         0.000000\n",
       "Name: finance, Length: 25876, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ensemble_ranking(score_g, score_l, rho):\n",
    "    exponent = 1 / rho\n",
    "    base = 0.5 * np.power(1/score_g, rho) + 0.5 * np.power(1/score_l, rho) \n",
    "    return 1/np.power(base, exponent)\n",
    "\n",
    "def scale_data(data):\n",
    "    scaler = MinMaxScaler()\n",
    "    return scaler.fit_transform(data)\n",
    "\n",
    "def compute_vocab_ensemble_rankings(score_l_df, score_g_df, vocab, rho=1): \n",
    "    raw = np.zeros((len(vocab), score_l_df.shape[1])).astype(np.double)\n",
    "    res = pd.DataFrame(raw, index=vocab, columns=score_g_df.columns)\n",
    "    for topic_idx in range(score_l_df.shape[1]):\n",
    "        score_l = score_l_df.loc[vocab, score_l_df.columns[topic_idx]]\n",
    "        score_g = score_g_df.loc[vocab, score_g_df.columns[topic_idx]]\n",
    "        res.iloc[:, topic_idx] = ensemble_ranking(score_l, score_g, rho)\n",
    "    return res\n",
    "s_global_cos_df = scale_data(global_cos_df)\n",
    "local_cos_df.loc[:, :] = scale_data(local_cos_df)\n",
    "global_cos_df.loc[:, :] = scale_data(global_cos_df)\n",
    "res = compute_vocab_ensemble_rankings(local_cos_df, global_cos_df, local_cos_df.index)\n",
    "res['finance'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2320390492677688\n",
      "-0.0146787450410294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.48877448630042725"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(global_cos_df.loc['sirikit', 'finance']), print(local_cos_df.loc['sirikit', 'accounting'])\n",
    "ensemble_ranking(global_cos_df.loc['demonstrated', 'finance'],local_cos_df.loc['demonstrated', 'accounting'], 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('honours')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "eea59d2da9fcf6d5a89ffa791376363fdbff07d1b7c37705fe62005135f4eda1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
